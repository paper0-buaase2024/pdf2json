[
    {
        "id": "2403.04445",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/"
    },
    {
        "abstract": "  Although remarkable progress on the neural table-to-text methods has been\nmade, the generalization issues hinder the applicability of these models due to\nthe limited source tables. Large-scale pretrained language models sound like a\npromising solution to tackle such issues. However, how to effectively bridge\nthe gap between the structured table and the text input by fully leveraging\ntable information to fuel the pretrained model is still not well explored.\nBesides, another challenge of integrating the deliberation mechanism into the\ntext-to-text pretrained model for solving the table-to-text task remains seldom\nstudied. In this paper, to implement the table-to-text generation with\npretrained language model, we propose a table structure understanding and text\ndeliberating approach, namely TASD. Specifically, we devise a three-layered\nmulti-head attention network to realize the table-structure-aware text\ngeneration model with the help of the pretrained language model. Furthermore, a\nmulti-pass decoder framework is adopted to enhance the capability of polishing\ngenerated text for table descriptions. The empirical studies, as well as human\nevaluation, on two public datasets, validate that our approach can generate\nfaithful and fluent descriptive texts for different types of tables.\n",
        "authors": "Miao Chen, Xinjiang Lu, Tong Xu, Yanyan Li, Jingbo Zhou, Dejing Dou,\n  Hui Xiong",
        "authors_parsed": [
            [
                "Chen",
                "Miao",
                ""
            ],
            [
                "Lu",
                "Xinjiang",
                ""
            ],
            [
                "Xu",
                "Tong",
                ""
            ],
            [
                "Li",
                "Yanyan",
                ""
            ],
            [
                "Zhou",
                "Jingbo",
                ""
            ],
            [
                "Dou",
                "Dejing",
                ""
            ],
            [
                "Xiong",
                "Hui",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI",
        "comments": null,
        "doi": null,
        "id": "2301.02071",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "report-no": null,
        "submitter": "Xinjiang Lu",
        "title": "Towards Table-to-Text Generation with Pretrained Language Model: A Table\n  Structure Understanding and Text Deliberating Approach",
        "update_date": "2023-01-06",
        "versions": [
            {
                "created": "Thu, 5 Jan 2023 14:03:26 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  This paper introduces PMIndiaSum, a multilingual and massively parallel\nsummarization corpus focused on languages in India. Our corpus provides a\ntraining and testing ground for four language families, 14 languages, and the\nlargest to date with 196 language pairs. We detail our construction workflow\nincluding data acquisition, processing, and quality assurance. Furthermore, we\npublish benchmarks for monolingual, cross-lingual, and multilingual\nsummarization by fine-tuning, prompting, as well as translate-and-summarize.\nExperimental results confirm the crucial role of our data in aiding\nsummarization between Indian languages. Our dataset is publicly available and\ncan be freely modified and re-distributed.\n",
        "authors": "Ashok Urlana, Pinzhen Chen, Zheng Zhao, Shay B. Cohen, Manish\n  Shrivastava, Barry Haddow",
        "authors_parsed": [
            [
                "Urlana",
                "Ashok",
                ""
            ],
            [
                "Chen",
                "Pinzhen",
                ""
            ],
            [
                "Zhao",
                "Zheng",
                ""
            ],
            [
                "Cohen",
                "Shay B.",
                ""
            ],
            [
                "Shrivastava",
                "Manish",
                ""
            ],
            [
                "Haddow",
                "Barry",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "Findings of EMNLP 2023",
        "doi": null,
        "id": "2305.08828",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Pinzhen Chen",
        "title": "PMIndiaSum: Multilingual and Cross-lingual Headline Summarization for\n  Languages in India",
        "update_date": "2023-10-23",
        "versions": [
            {
                "created": "Mon, 15 May 2023 17:41:15 GMT",
                "version": "v1"
            },
            {
                "created": "Fri, 20 Oct 2023 00:42:27 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  An extensive library of symptom inventories has been developed over time to\nmeasure clinical symptoms, but this variety has led to several long standing\nissues. Most notably, results drawn from different settings and studies are not\ncomparable, which limits reproducibility. Here, we present an artificial\nintelligence (AI) approach using semantic textual similarity (STS) to link\nsymptoms and scores across previously incongruous symptom inventories. We\ntested the ability of four pre-trained STS models to screen thousands of\nsymptom description pairs for related content - a challenging task typically\nrequiring expert panels. Models were tasked to predict symptom severity across\nfour different inventories for 6,607 participants drawn from 16 international\ndata sources. The STS approach achieved 74.8% accuracy across five tasks,\noutperforming other models tested. This work suggests that incorporating\ncontextual, semantic information can assist expert decision-making processes,\nyielding gains for both general and disease-specific clinical assessment.\n",
        "authors": "Eamonn Kennedy, Shashank Vadlamani, Hannah M Lindsey, Kelly S\n  Peterson, Kristen Dams OConnor, Kenton Murray, Ronak Agarwal, Houshang H\n  Amiri, Raeda K Andersen, Talin Babikian, David A Baron, Erin D Bigler, Karen\n  Caeyenberghs, Lisa Delano-Wood, Seth G Disner, Ekaterina Dobryakova, Blessen\n  C Eapen, Rachel M Edelstein, Carrie Esopenko, Helen M Genova, Elbert Geuze,\n  Naomi J Goodrich-Hunsaker, Jordan Grafman, Asta K Haberg, Cooper B Hodges,\n  Kristen R Hoskinson, Elizabeth S Hovenden, Andrei Irimia, Neda Jahanshad,\n  Ruchira M Jha, Finian Keleher, Kimbra Kenney, Inga K Koerte, Spencer W\n  Liebel, Abigail Livny, Marianne Lovstad, Sarah L Martindale, Jeffrey E Max,\n  Andrew R Mayer, Timothy B Meier, Deleene S Menefee, Abdalla Z Mohamed,\n  Stefania Mondello, Martin M Monti, Rajendra A Morey, Virginia Newcombe, Mary\n  R Newsome, Alexander Olsen, Nicholas J Pastorek, Mary Jo Pugh, Adeel Razi,\n  Jacob E Resch, Jared A Rowland, Kelly Russell, Nicholas P Ryan, Randall S\n  Scheibel, Adam T Schmidt, Gershon Spitz, Jaclyn A Stephens, Assaf Tal, Leah D\n  Talbert, Maria Carmela Tartaglia, Brian A Taylor, Sophia I Thomopoulos, Maya\n  Troyanskaya, Eve M Valera, Harm Jan van der Horn, John D Van Horn, Ragini\n  Verma, Benjamin SC Wade, Willian SC Walker, Ashley L Ware, J Kent Werner Jr,\n  Keith Owen Yeates, Ross D Zafonte, Michael M Zeineh, Brandon Zielinski, Paul\n  M Thompson, Frank G Hillary, David F Tate, Elisabeth A Wilde, Emily L Dennis",
        "authors_parsed": [
            [
                "Kennedy",
                "Eamonn",
                ""
            ],
            [
                "Vadlamani",
                "Shashank",
                ""
            ],
            [
                "Lindsey",
                "Hannah M",
                ""
            ],
            [
                "Peterson",
                "Kelly S",
                ""
            ],
            [
                "OConnor",
                "Kristen Dams",
                ""
            ],
            [
                "Murray",
                "Kenton",
                ""
            ],
            [
                "Agarwal",
                "Ronak",
                ""
            ],
            [
                "Amiri",
                "Houshang H",
                ""
            ],
            [
                "Andersen",
                "Raeda K",
                ""
            ],
            [
                "Babikian",
                "Talin",
                ""
            ],
            [
                "Baron",
                "David A",
                ""
            ],
            [
                "Bigler",
                "Erin D",
                ""
            ],
            [
                "Caeyenberghs",
                "Karen",
                ""
            ],
            [
                "Delano-Wood",
                "Lisa",
                ""
            ],
            [
                "Disner",
                "Seth G",
                ""
            ],
            [
                "Dobryakova",
                "Ekaterina",
                ""
            ],
            [
                "Eapen",
                "Blessen C",
                ""
            ],
            [
                "Edelstein",
                "Rachel M",
                ""
            ],
            [
                "Esopenko",
                "Carrie",
                ""
            ],
            [
                "Genova",
                "Helen M",
                ""
            ],
            [
                "Geuze",
                "Elbert",
                ""
            ],
            [
                "Goodrich-Hunsaker",
                "Naomi J",
                ""
            ],
            [
                "Grafman",
                "Jordan",
                ""
            ],
            [
                "Haberg",
                "Asta K",
                ""
            ],
            [
                "Hodges",
                "Cooper B",
                ""
            ],
            [
                "Hoskinson",
                "Kristen R",
                ""
            ],
            [
                "Hovenden",
                "Elizabeth S",
                ""
            ],
            [
                "Irimia",
                "Andrei",
                ""
            ],
            [
                "Jahanshad",
                "Neda",
                ""
            ],
            [
                "Jha",
                "Ruchira M",
                ""
            ],
            [
                "Keleher",
                "Finian",
                ""
            ],
            [
                "Kenney",
                "Kimbra",
                ""
            ],
            [
                "Koerte",
                "Inga K",
                ""
            ],
            [
                "Liebel",
                "Spencer W",
                ""
            ],
            [
                "Livny",
                "Abigail",
                ""
            ],
            [
                "Lovstad",
                "Marianne",
                ""
            ],
            [
                "Martindale",
                "Sarah L",
                ""
            ],
            [
                "Max",
                "Jeffrey E",
                ""
            ],
            [
                "Mayer",
                "Andrew R",
                ""
            ],
            [
                "Meier",
                "Timothy B",
                ""
            ],
            [
                "Menefee",
                "Deleene S",
                ""
            ],
            [
                "Mohamed",
                "Abdalla Z",
                ""
            ],
            [
                "Mondello",
                "Stefania",
                ""
            ],
            [
                "Monti",
                "Martin M",
                ""
            ],
            [
                "Morey",
                "Rajendra A",
                ""
            ],
            [
                "Newcombe",
                "Virginia",
                ""
            ],
            [
                "Newsome",
                "Mary R",
                ""
            ],
            [
                "Olsen",
                "Alexander",
                ""
            ],
            [
                "Pastorek",
                "Nicholas J",
                ""
            ],
            [
                "Pugh",
                "Mary Jo",
                ""
            ],
            [
                "Razi",
                "Adeel",
                ""
            ],
            [
                "Resch",
                "Jacob E",
                ""
            ],
            [
                "Rowland",
                "Jared A",
                ""
            ],
            [
                "Russell",
                "Kelly",
                ""
            ],
            [
                "Ryan",
                "Nicholas P",
                ""
            ],
            [
                "Scheibel",
                "Randall S",
                ""
            ],
            [
                "Schmidt",
                "Adam T",
                ""
            ],
            [
                "Spitz",
                "Gershon",
                ""
            ],
            [
                "Stephens",
                "Jaclyn A",
                ""
            ],
            [
                "Tal",
                "Assaf",
                ""
            ],
            [
                "Talbert",
                "Leah D",
                ""
            ],
            [
                "Tartaglia",
                "Maria Carmela",
                ""
            ],
            [
                "Taylor",
                "Brian A",
                ""
            ],
            [
                "Thomopoulos",
                "Sophia I",
                ""
            ],
            [
                "Troyanskaya",
                "Maya",
                ""
            ],
            [
                "Valera",
                "Eve M",
                ""
            ],
            [
                "van der Horn",
                "Harm Jan",
                ""
            ],
            [
                "Van Horn",
                "John D",
                ""
            ],
            [
                "Verma",
                "Ragini",
                ""
            ],
            [
                "Wade",
                "Benjamin SC",
                ""
            ],
            [
                "Walker",
                "Willian SC",
                ""
            ],
            [
                "Ware",
                "Ashley L",
                ""
            ],
            [
                "Werner",
                "J Kent",
                "Jr"
            ],
            [
                "Yeates",
                "Keith Owen",
                ""
            ],
            [
                "Zafonte",
                "Ross D",
                ""
            ],
            [
                "Zeineh",
                "Michael M",
                ""
            ],
            [
                "Zielinski",
                "Brandon",
                ""
            ],
            [
                "Thompson",
                "Paul M",
                ""
            ],
            [
                "Hillary",
                "Frank G",
                ""
            ],
            [
                "Tate",
                "David F",
                ""
            ],
            [
                "Wilde",
                "Elisabeth A",
                ""
            ],
            [
                "Dennis",
                "Emily L",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI",
        "comments": null,
        "doi": null,
        "id": "2309.04607",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "report-no": null,
        "submitter": "Emily Dennis",
        "title": "Linking Symptom Inventories using Semantic Textual Similarity",
        "update_date": "2023-09-12",
        "versions": [
            {
                "created": "Fri, 8 Sep 2023 21:50:10 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Little research has explored how information engagement (IE), the degree to\nwhich individuals interact with and use information in a manner that manifests\ncognitively, behaviorally, and affectively. This study explored the impact of\nphrasing, specifically word choice, on IE and decision making. Synthesizing two\ntheoretical models, User Engagement Theory UET and Information Behavior Theory\nIBT, a theoretical framework illustrating the impact of and relationships among\nthe three IE dimensions of perception, participation, and perseverance was\ndeveloped and hypotheses generated. The framework was empirically validated in\na large-scale user study measuring how word choice impacts the dimensions of\nIE. The findings provide evidence that IE differs from other forms of\nengagement in that it is driven and fostered by the expression of the\ninformation itself, regardless of the information system used to view, interact\nwith, and use the information. The findings suggest that phrasing can have a\nsignificant effect on the interpretation of and interaction with digital\ninformation, indicating the importance of expression of information, in\nparticular word choice, on decision making and IE. The research contributes to\nthe literature by identifying methods for assessment and improvement of IE and\ndecision making with digital text.\n",
        "authors": "Nimrod Dvir, Elaine Friedman, Suraj Commuri, Fan Yang, Jennifer Romano",
        "authors_parsed": [
            [
                "Dvir",
                "Nimrod",
                ""
            ],
            [
                "Friedman",
                "Elaine",
                ""
            ],
            [
                "Commuri",
                "Suraj",
                ""
            ],
            [
                "Yang",
                "Fan",
                ""
            ],
            [
                "Romano",
                "Jennifer",
                ""
            ]
        ],
        "categories": "cs.CL cs.HC cs.SY eess.SY stat.AP",
        "comments": null,
        "doi": null,
        "id": "2305.09798",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Nimrod Dvir Dr.",
        "title": "The Ways of Words: The Impact of Word Choice on Information Engagement\n  and Decision Making",
        "update_date": "2023-05-18",
        "versions": [
            {
                "created": "Tue, 16 May 2023 20:46:36 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Pre-trained language models based on general text enable huge success in the\nNLP scenario. But the intrinsical difference of linguistic patterns between\ngeneral text and task-oriented dialogues makes existing pre-trained language\nmodels less useful in practice. Current dialogue pre-training methods rely on a\ncontrastive framework and face the challenges of both selecting true positives\nand hard negatives. In this paper, we propose a novel dialogue pre-training\nmodel, FutureTOD, which distills future knowledge to the representation of the\nprevious dialogue context using a self-training framework. Our intuition is\nthat a good dialogue representation both learns local context information and\npredicts future information. Extensive experiments on diverse downstream\ndialogue tasks demonstrate the effectiveness of our model, especially the\ngeneralization, robustness, and learning discriminative dialogue\nrepresentations capabilities.\n",
        "authors": "Weihao Zeng, Keqing He, Yejie Wang, Chen Zeng, Jingang Wang, Yunsen\n  Xian, Weiran Xu",
        "authors_parsed": [
            [
                "Zeng",
                "Weihao",
                ""
            ],
            [
                "He",
                "Keqing",
                ""
            ],
            [
                "Wang",
                "Yejie",
                ""
            ],
            [
                "Zeng",
                "Chen",
                ""
            ],
            [
                "Wang",
                "Jingang",
                ""
            ],
            [
                "Xian",
                "Yunsen",
                ""
            ],
            [
                "Xu",
                "Weiran",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "ACL 2023 Main Conference",
        "doi": null,
        "id": "2306.10315",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Weihao Zeng",
        "title": "FutureTOD: Teaching Future Knowledge to Pre-trained Language Model for\n  Task-Oriented Dialogue",
        "update_date": "2023-06-21",
        "versions": [
            {
                "created": "Sat, 17 Jun 2023 10:40:07 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Human communication is a complex and diverse process that not only involves\nmultiple factors such as language, commonsense, and cultural backgrounds but\nalso requires the participation of multimodal information, such as speech.\nLarge Language Model (LLM)-based multi-agent systems have demonstrated\npromising performance in simulating human society. Can we leverage LLM-based\nmulti-agent systems to simulate human communication? However, current LLM-based\nmulti-agent systems mainly rely on text as the primary medium. In this paper,\nwe propose SpeechAgents, a multi-modal LLM based multi-agent system designed\nfor simulating human communication. SpeechAgents utilizes multi-modal LLM as\nthe control center for individual agent and employes multi-modal signals as the\nmedium for exchanged messages among agents. Additionally, we propose\nMulti-Agent Tuning to enhance the multi-agent capabilities of LLM without\ncompromising general abilities. To strengthen and evaluate the effectiveness of\nhuman communication simulation, we build the Human-Communication Simulation\nBenchmark. Experimental results demonstrate that SpeechAgents can simulate\nhuman communication dialogues with consistent content, authentic rhythm, and\nrich emotions and demonstrate excellent scalability even with up to 25 agents,\nwhich can apply to tasks such as drama creation and audio novels generation.\nCode and models will be open-sourced at https://github.\ncom/0nutation/SpeechAgents\n",
        "authors": "Dong Zhang, Zhaowei Li, Pengyu Wang, Xin Zhang, Yaqian Zhou, Xipeng\n  Qiu",
        "authors_parsed": [
            [
                "Zhang",
                "Dong",
                ""
            ],
            [
                "Li",
                "Zhaowei",
                ""
            ],
            [
                "Wang",
                "Pengyu",
                ""
            ],
            [
                "Zhang",
                "Xin",
                ""
            ],
            [
                "Zhou",
                "Yaqian",
                ""
            ],
            [
                "Qiu",
                "Xipeng",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "work in progress",
        "doi": null,
        "id": "2401.03945",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "report-no": null,
        "submitter": "Dong Zhang",
        "title": "SpeechAgents: Human-Communication Simulation with Multi-Modal\n  Multi-Agent Systems",
        "update_date": "2024-01-09",
        "versions": [
            {
                "created": "Mon, 8 Jan 2024 15:01:08 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  In recent years, End-to-End speech recognition technology based on deep\nlearning has developed rapidly. Due to the lack of Turkish speech data, the\nperformance of Turkish speech recognition system is poor. Firstly, this paper\nstudies a series of speech recognition tuning technologies. The results show\nthat the performance of the model is the best when the data enhancement\ntechnology combining speed perturbation with noise addition is adopted and the\nbeam search width is set to 16. Secondly, to maximize the use of effective\nfeature information and improve the accuracy of feature extraction, this paper\nproposes a new feature extractor LSPC. LSPC and LiGRU network are combined to\nform a shared encoder structure, and model compression is realized. The results\nshow that the performance of LSPC is better than MSPC and VGGnet when only\nusing Fbank features, and the WER is improved by 1.01% and 2.53% respectively.\nFinally, based on the above two points, a new multi-feature fusion network is\nproposed as the main structure of the encoder. The results show that the WER of\nthe proposed feature fusion network based on LSPC is improved by 0.82% and\n1.94% again compared with the single feature (Fbank feature and Spectrogram\nfeature) extraction using LSPC. Our model achieves performance comparable to\nthat of advanced End-to-End models.\n",
        "authors": "Zeyu Ren, Nurmement Yolwas, Huiru Wang, Wushour Slamu",
        "authors_parsed": [
            [
                "Ren",
                "Zeyu",
                ""
            ],
            [
                "Yolwas",
                "Nurmement",
                ""
            ],
            [
                "Wang",
                "Huiru",
                ""
            ],
            [
                "Slamu",
                "Wushour",
                ""
            ]
        ],
        "categories": "cs.SD cs.CL eess.AS",
        "comments": null,
        "doi": null,
        "id": "2303.12300",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Zeyu Ren",
        "title": "Exploring Turkish Speech Recognition via Hybrid CTC/Attention\n  Architecture and Multi-feature Fusion Network",
        "update_date": "2023-03-23",
        "versions": [
            {
                "created": "Wed, 22 Mar 2023 04:11:35 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  The acquisition of grammar has been a central question to adjudicate between\ntheories of language acquisition. In order to conduct faster, more\nreproducible, and larger-scale corpus studies on grammaticality in\nchild-caregiver conversations, tools for automatic annotation can offer an\neffective alternative to tedious manual annotation. We propose a coding scheme\nfor context-dependent grammaticality in child-caregiver conversations and\nannotate more than 4,000 utterances from a large corpus of transcribed\nconversations. Based on these annotations, we train and evaluate a range of NLP\nmodels. Our results show that fine-tuned Transformer-based models perform best,\nachieving human inter-annotation agreement levels.As a first application and\nsanity check of this tool, we use the trained models to annotate a corpus\nalmost two orders of magnitude larger than the manually annotated data and\nverify that children's grammaticality shows a steady increase with age.This\nwork contributes to the growing literature on applying state-of-the-art NLP\nmethods to help study child language acquisition at scale.\n",
        "authors": "Mitja Nikolaus (ILCB, LPL, LIS, TALEP), Abhishek Agrawal (ILCB, LIS,\n  TALEP), Petros Kaklamanis, Alex Warstadt (SED), Abdellah Fourtassi (ILCB,\n  LIS, TALEP)",
        "authors_parsed": [
            [
                "Nikolaus",
                "Mitja",
                "",
                "ILCB, LPL, LIS, TALEP"
            ],
            [
                "Agrawal",
                "Abhishek",
                "",
                "ILCB, LIS,\n  TALEP"
            ],
            [
                "Kaklamanis",
                "Petros",
                "",
                "SED"
            ],
            [
                "Warstadt",
                "Alex",
                "",
                "SED"
            ],
            [
                "Fourtassi",
                "Abdellah",
                "",
                "ILCB,\n  LIS, TALEP"
            ]
        ],
        "categories": "cs.CL",
        "comments": null,
        "doi": null,
        "id": "2403.14208",
        "journal-ref": "LREC-Coling 2024, May 2024, Turin, Italy",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Mitja Nikolaus",
        "title": "Automatic Annotation of Grammaticality in Child-Caregiver Conversations",
        "update_date": "2024-03-22",
        "versions": [
            {
                "created": "Thu, 21 Mar 2024 08:00:05 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Social media is a modern person's digital voice to project and engage with\nnew ideas and mobilise communities $\\unicode{x2013}$ a power shared with\nextremists. Given the societal risks of unvetted content-moderating algorithms\nfor Extremism, Radicalisation, and Hate speech (ERH) detection, responsible\nsoftware engineering must understand the who, what, when, where, and why such\nmodels are necessary to protect user safety and free expression. Hence, we\npropose and examine the unique research field of ERH context mining to unify\ndisjoint studies. Specifically, we evaluate the start-to-finish design process\nfrom socio-technical definition-building and dataset collection strategies to\ntechnical algorithm design and performance. Our 2015-2021 51-study Systematic\nLiterature Review (SLR) provides the first cross-examination of textual,\nnetwork, and visual approaches to detecting extremist affiliation, hateful\ncontent, and radicalisation towards groups and movements. We identify\nconsensus-driven ERH definitions and propose solutions to existing ideological\nand geographic biases, particularly due to the lack of research in\nOceania/Australasia. Our hybridised investigation on Natural Language\nProcessing, Community Detection, and visual-text models demonstrates the\ndominating performance of textual transformer-based algorithms. We conclude\nwith vital recommendations for ERH context mining researchers and propose an\nuptake roadmap with guidelines for researchers, industries, and governments to\nenable a safer cyberspace.\n",
        "authors": "Jarod Govers, Philip Feldman, Aaron Dant, Panos Patros",
        "authors_parsed": [
            [
                "Govers",
                "Jarod",
                ""
            ],
            [
                "Feldman",
                "Philip",
                ""
            ],
            [
                "Dant",
                "Aaron",
                ""
            ],
            [
                "Patros",
                "Panos",
                ""
            ]
        ],
        "categories": "cs.SI cs.AI cs.CL cs.CY cs.LG",
        "comments": "35-page main literature review, 14-page supplementary material.\n  Submitted to ACM Computing Surveys (Dec 2021)",
        "doi": null,
        "id": "2301.11579",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Jarod Govers",
        "title": "Down the Rabbit Hole: Detecting Online Extremism, Radicalisation, and\n  Politicised Hate Speech",
        "update_date": "2023-01-30",
        "versions": [
            {
                "created": "Fri, 27 Jan 2023 07:59:31 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Designing de novo proteins beyond those found in nature holds significant\npromise for advancements in both scientific and engineering applications.\nCurrent methodologies for protein design often rely on AI-based models, such as\nsurrogate models that address end-to-end problems by linking protein structure\nto material properties or vice versa. However, these models frequently focus on\nspecific material objectives or structural properties, limiting their\nflexibility when incorporating out-of-domain knowledge into the design process\nor comprehensive data analysis is required. In this study, we introduce\nProtAgents, a platform for de novo protein design based on Large Language\nModels (LLMs), where multiple AI agents with distinct capabilities\ncollaboratively address complex tasks within a dynamic environment. The\nversatility in agent development allows for expertise in diverse domains,\nincluding knowledge retrieval, protein structure analysis, physics-based\nsimulations, and results analysis. The dynamic collaboration between agents,\nempowered by LLMs, provides a versatile approach to tackling protein design and\nanalysis problems, as demonstrated through diverse examples in this study. The\nproblems of interest encompass designing new proteins, analyzing protein\nstructures and obtaining new first-principles data -- natural vibrational\nfrequencies -- via physics simulations. The concerted effort of the system\nallows for powerful automated and synergistic design of de novo proteins with\ntargeted mechanical properties. The flexibility in designing the agents, on one\nhand, and their capacity in autonomous collaboration through the dynamic\nLLM-based multi-agent environment on the other hand, unleashes great potentials\nof LLMs in addressing multi-objective materials problems and opens up new\navenues for autonomous materials discovery and design.\n",
        "authors": "A. Ghafarollahi, M.J. Buehler",
        "authors_parsed": [
            [
                "Ghafarollahi",
                "A.",
                ""
            ],
            [
                "Buehler",
                "M. J.",
                ""
            ]
        ],
        "categories": "cond-mat.soft cs.AI cs.CL q-bio.BM",
        "comments": null,
        "doi": null,
        "id": "2402.04268",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "report-no": null,
        "submitter": "Markus Buehler",
        "title": "ProtAgents: Protein discovery via large language model multi-agent\n  collaborations combining physics and machine learning",
        "update_date": "2024-02-08",
        "versions": [
            {
                "created": "Sat, 27 Jan 2024 20:19:49 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  While multilingual language models can improve NLP performance on\nlow-resource languages by leveraging higher-resource languages, they also\nreduce average performance on all languages (the 'curse of multilinguality').\nHere we show another problem with multilingual models: grammatical structures\nin higher-resource languages bleed into lower-resource languages, a phenomenon\nwe call grammatical structure bias. We show this bias via a novel method for\ncomparing the fluency of multilingual models to the fluency of monolingual\nSpanish and Greek models: testing their preference for two carefully-chosen\nvariable grammatical structures (optional pronoun-drop in Spanish and optional\nSubject-Verb ordering in Greek). We find that multilingual BERT is biased\ntoward the English-like setting (explicit pronouns and Subject-Verb-Object\nordering) as compared to our monolingual control language model. With our case\nstudies, we hope to bring to light the fine-grained ways in which multilingual\nmodels can be biased,and encourage more linguistically-aware fluency\nevaluation.\n",
        "authors": "Isabel Papadimitriou, Kezia Lopez, Dan Jurafsky",
        "authors_parsed": [
            [
                "Papadimitriou",
                "Isabel",
                ""
            ],
            [
                "Lopez",
                "Kezia",
                ""
            ],
            [
                "Jurafsky",
                "Dan",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "Findings of EACL 2023",
        "doi": null,
        "id": "2210.05619",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Isabel Papadimitriou",
        "title": "Multilingual BERT has an accent: Evaluating English influences on\n  fluency in multilingual models",
        "update_date": "2023-04-14",
        "versions": [
            {
                "created": "Tue, 11 Oct 2022 17:06:38 GMT",
                "version": "v1"
            },
            {
                "created": "Thu, 13 Apr 2023 14:59:38 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Large language models (LLMs) have shown remarkable capabilities in generating\nhigh-quality text and making predictions based on large amounts of data,\nincluding the media domain. However, in practical applications, the differences\nbetween the media's use cases and the general-purpose applications of LLMs have\nbecome increasingly apparent, especially Chinese. This paper examines the\nunique characteristics of media-domain-specific LLMs compared to general LLMs,\ndesigned a diverse set of task instruction types to cater the specific\nrequirements of the domain and constructed unique datasets that are tailored to\nthe media domain. Based on these, we proposed MediaGPT, a domain-specific LLM\nfor the Chinese media domain, training by domain-specific data and experts SFT\ndata. By performing human experts evaluation and strong model evaluation on a\nvalidation set, this paper demonstrated that MediaGPT outperforms mainstream\nmodels on various Chinese media domain tasks and verifies the importance of\ndomain data and domain-defined prompt types for building an effective\ndomain-specific LLM.\n",
        "authors": "Zhonghao Wang, Zijia Lu, Bo Jin, Haiying Deng",
        "authors_parsed": [
            [
                "Wang",
                "Zhonghao",
                ""
            ],
            [
                "Lu",
                "Zijia",
                ""
            ],
            [
                "Jin",
                "Bo",
                ""
            ],
            [
                "Deng",
                "Haiying",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI",
        "comments": null,
        "doi": null,
        "id": "2307.10930",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Zhonghao Wang",
        "title": "MediaGPT : A Large Language Model For Chinese Media",
        "update_date": "2023-07-27",
        "versions": [
            {
                "created": "Thu, 20 Jul 2023 14:59:02 GMT",
                "version": "v1"
            },
            {
                "created": "Wed, 26 Jul 2023 14:21:47 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Multi-modal large language models have garnered significant interest\nrecently. Though, most of the works focus on vision-language multi-modal models\nproviding strong capabilities in following vision-and-language instructions.\nHowever, we claim that speech is also an important modality through which\nhumans interact with the world. Hence, it is crucial for a general-purpose\nassistant to be able to follow multi-modal speech-and-language instructions. In\nthis work, we propose Large Language and Speech Model (LLaSM). LLaSM is an\nend-to-end trained large multi-modal speech-language model with cross-modal\nconversational abilities, capable of following speech-and-language\ninstructions. Our early experiments show that LLaSM demonstrates a more\nconvenient and natural way for humans to interact with artificial intelligence.\nSpecifically, we also release a large Speech Instruction Following dataset\nLLaSM-Audio-Instructions. Code and demo are available at\nhttps://github.com/LinkSoul-AI/LLaSM and\nhttps://huggingface.co/spaces/LinkSoul/LLaSM. The LLaSM-Audio-Instructions\ndataset is available at\nhttps://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.\n",
        "authors": "Yu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen\n  Shi, Qiqi Xiang, Yemin Shi",
        "authors_parsed": [
            [
                "Shu",
                "Yu",
                ""
            ],
            [
                "Dong",
                "Siwei",
                ""
            ],
            [
                "Chen",
                "Guangyao",
                ""
            ],
            [
                "Huang",
                "Wenhao",
                ""
            ],
            [
                "Zhang",
                "Ruihua",
                ""
            ],
            [
                "Shi",
                "Daochen",
                ""
            ],
            [
                "Xiang",
                "Qiqi",
                ""
            ],
            [
                "Shi",
                "Yemin",
                ""
            ]
        ],
        "categories": "cs.CL cs.LG cs.SD eess.AS",
        "comments": null,
        "doi": null,
        "id": "2308.15930",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Yu Shu",
        "title": "LLaSM: Large Language and Speech Model",
        "update_date": "2023-09-19",
        "versions": [
            {
                "created": "Wed, 30 Aug 2023 10:12:39 GMT",
                "version": "v1"
            },
            {
                "created": "Tue, 12 Sep 2023 03:41:35 GMT",
                "version": "v2"
            },
            {
                "created": "Sat, 16 Sep 2023 06:14:54 GMT",
                "version": "v3"
            }
        ]
    },
    {
        "abstract": "  Quality and diversity have been proposed as reasonable heuristics for\nassessing content generated by co-creative systems, but to date there has been\nlittle agreement around what constitutes the latter or how to measure it.\nProposed approaches for assessing generative models in terms of diversity have\nlimitations in that they compare the model's outputs to a ground truth that in\nthe era of large pre-trained generative models might not be available, or\nentail an impractical number of computations. We propose an alternative based\non entropy of neural network encodings for comparing diversity between sets of\nimages that does not require ground-truth knowledge and is easy to compute. We\nalso compare two pre-trained networks and show how the choice relates to the\nnotion of diversity that we want to evaluate. We conclude with a discussion of\nthe potential applications of these measures for ideation in interactive\nsystems, model evaluation, and more broadly within computational creativity.\n",
        "authors": "Francisco Ibarrola and Kazjon Grace",
        "authors_parsed": [
            [
                "Ibarrola",
                "Francisco",
                ""
            ],
            [
                "Grace",
                "Kazjon",
                ""
            ]
        ],
        "categories": "cs.CV cs.CL cs.LG",
        "comments": null,
        "doi": null,
        "id": "2403.13826",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Francisco Ibarrola",
        "title": "Measuring Diversity in Co-creative Image Generation",
        "update_date": "2024-03-22",
        "versions": [
            {
                "created": "Wed, 6 Mar 2024 01:55:14 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Aspect-Based Sentiment Analysis (ABSA) stands as a crucial task in predicting\nthe sentiment polarity associated with identified aspects within text. However,\na notable challenge in ABSA lies in precisely determining the aspects'\nboundaries (start and end indices), especially for long ones, due to users'\ncolloquial expressions. We propose DiffusionABSA, a novel diffusion model\ntailored for ABSA, which extracts the aspects progressively step by step.\nParticularly, DiffusionABSA gradually adds noise to the aspect terms in the\ntraining process, subsequently learning a denoising process that progressively\nrestores these terms in a reverse manner. To estimate the boundaries, we design\na denoising neural network enhanced by a syntax-aware temporal attention\nmechanism to chronologically capture the interplay between aspects and\nsurrounding text. Empirical evaluations conducted on eight benchmark datasets\nunderscore the compelling advantages offered by DiffusionABSA when compared\nagainst robust baseline models. Our code is publicly available at\nhttps://github.com/Qlb6x/DiffusionABSA.\n",
        "authors": "Shunyu Liu, Jie Zhou, Qunxi Zhu, Qin Chen, Qingchun Bai, Jun Xiao,\n  Liang He",
        "authors_parsed": [
            [
                "Liu",
                "Shunyu",
                ""
            ],
            [
                "Zhou",
                "Jie",
                ""
            ],
            [
                "Zhu",
                "Qunxi",
                ""
            ],
            [
                "Chen",
                "Qin",
                ""
            ],
            [
                "Bai",
                "Qingchun",
                ""
            ],
            [
                "Xiao",
                "Jun",
                ""
            ],
            [
                "He",
                "Liang",
                ""
            ]
        ],
        "categories": "cs.CL cs.LG",
        "comments": "Accepted to LREC-COLING 2024, submission version",
        "doi": null,
        "id": "2402.15289",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Shunyu Liu",
        "title": "Let's Rectify Step by Step: Improving Aspect-based Sentiment Analysis\n  with Diffusion Models",
        "update_date": "2024-02-26",
        "versions": [
            {
                "created": "Fri, 23 Feb 2024 12:35:43 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Intuitive psychology is a pillar of common-sense reasoning. The replication\nof this reasoning in machine intelligence is an important stepping-stone on the\nway to human-like artificial intelligence. Several recent tasks and benchmarks\nfor examining this reasoning in Large-Large Models have focused in particular\non belief attribution in Theory-of-Mind tasks. These tasks have shown both\nsuccesses and failures. We consider in particular a recent purported success\ncase, and show that small variations that maintain the principles of ToM turn\nthe results on their head. We argue that in general, the zero-hypothesis for\nmodel evaluation in intuitive psychology should be skeptical, and that outlying\nfailure cases should outweigh average success rates. We also consider what\npossible future successes on Theory-of-Mind tasks by more powerful LLMs would\nmean for ToM tasks with people.\n",
        "authors": "Tomer Ullman",
        "authors_parsed": [
            [
                "Ullman",
                "Tomer",
                ""
            ]
        ],
        "categories": "cs.AI cs.CL",
        "comments": "11 pages, 2 figures",
        "doi": null,
        "id": "2302.08399",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Tomer Ullman",
        "title": "Large Language Models Fail on Trivial Alterations to Theory-of-Mind\n  Tasks",
        "update_date": "2023-03-15",
        "versions": [
            {
                "created": "Thu, 16 Feb 2023 16:18:03 GMT",
                "version": "v1"
            },
            {
                "created": "Fri, 17 Feb 2023 15:34:34 GMT",
                "version": "v2"
            },
            {
                "created": "Mon, 20 Feb 2023 03:46:17 GMT",
                "version": "v3"
            },
            {
                "created": "Sun, 26 Feb 2023 18:18:38 GMT",
                "version": "v4"
            },
            {
                "created": "Tue, 14 Mar 2023 13:47:26 GMT",
                "version": "v5"
            }
        ]
    },
    {
        "abstract": "  Pre-training on large-scale open-domain dialogue data can substantially\nimprove the performance of dialogue models. However, the pre-trained dialogue\nmodel's ability to utilize long-range context is limited due to the scarcity of\nlong-turn dialogue sessions. Most dialogues in existing pre-training corpora\ncontain fewer than three turns of dialogue. To alleviate this issue, we propose\nthe Retrieve, Reorganize and Rescale framework (Re$^3$Dial), which can\nautomatically construct billion-scale long-turn dialogues by reorganizing\nexisting short-turn ones. Given a short-turn session, Re$^3$Dial first employs\na session retriever to retrieve coherent consecutive sessions. To this end, we\ntrain the retriever to capture semantic and discourse relations within\nmulti-turn dialogues through contrastive training. Next, Re$^3$Dial samples a\nsession from retrieved results following a diversity sampling strategy, which\nis designed to penalize repetitive or generic sessions. A longer session is\nthen derived by concatenating the original session and the sampled session. By\nrepeating the above process, Re$^3$Dial can yield a coherent long-turn\ndialogue. Extensive experiments on multiple multi-turn dialogue benchmarks\ndemonstrate that Re$^3$Dial significantly improves the dialogue model's ability\nto utilize long-range context and thus generate more sensible and informative\nresponses. Finally, we build a toolkit for efficiently rescaling conversations\nwith Re$^3$Dial, which enables us to construct a corpus containing 1B Chinese\ndialogue sessions with 11.3 turns on average (5$\\times$ longer than the\noriginal corpus). Our retriever model, code, and data is publicly available at\n\\url{https://github.com/thu-coai/Re3Dial}.\n",
        "authors": "Jiaxin Wen, Hao Zhou, Jian Guan, Minlie Huang",
        "authors_parsed": [
            [
                "Wen",
                "Jiaxin",
                ""
            ],
            [
                "Zhou",
                "Hao",
                ""
            ],
            [
                "Guan",
                "Jian",
                ""
            ],
            [
                "Huang",
                "Minlie",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "EMNLP 2023 Main Coference",
        "doi": null,
        "id": "2305.02606",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Jiaxin Wen",
        "title": "Re$^3$Dial: Retrieve, Reorganize and Rescale Dialogue Corpus for\n  Long-Turn Open-Domain Dialogue Pre-training",
        "update_date": "2023-10-24",
        "versions": [
            {
                "created": "Thu, 4 May 2023 07:28:23 GMT",
                "version": "v1"
            },
            {
                "created": "Sun, 22 Oct 2023 07:41:06 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Despite their wide-spread success, Text-to-Image models (T2I) still struggle\nto produce images that are both aesthetically pleasing and faithful to the\nuser's input text. We introduce DreamSync, a model-agnostic training algorithm\nby design that improves T2I models to be faithful to the text input. DreamSync\nbuilds off a recent insight from TIFA's evaluation framework -- that large\nvision-language models (VLMs) can effectively identify the fine-grained\ndiscrepancies between generated images and the text inputs. DreamSync uses this\ninsight to train T2I models without any labeled data; it improves T2I models\nusing its own generations. First, it prompts the model to generate several\ncandidate images for a given input text. Then, it uses two VLMs to select the\nbest generation: a Visual Question Answering model that measures the alignment\nof generated images to the text, and another that measures the generation's\naesthetic quality. After selection, we use LoRA to iteratively finetune the T2I\nmodel to guide its generation towards the selected best generations. DreamSync\ndoes not need any additional human annotation. model architecture changes, or\nreinforcement learning. Despite its simplicity, DreamSync improves both the\nsemantic alignment and aesthetic appeal of two diffusion-based T2I models,\nevidenced by multiple benchmarks (+1.7% on TIFA, +2.9% on DSG1K, +3.4% on VILA\naesthetic) and human evaluation.\n",
        "authors": "Jiao Sun, Deqing Fu, Yushi Hu, Su Wang, Royi Rassin, Da-Cheng Juan,\n  Dana Alon, Charles Herrmann, Sjoerd van Steenkiste, Ranjay Krishna, Cyrus\n  Rashtchian",
        "authors_parsed": [
            [
                "Sun",
                "Jiao",
                ""
            ],
            [
                "Fu",
                "Deqing",
                ""
            ],
            [
                "Hu",
                "Yushi",
                ""
            ],
            [
                "Wang",
                "Su",
                ""
            ],
            [
                "Rassin",
                "Royi",
                ""
            ],
            [
                "Juan",
                "Da-Cheng",
                ""
            ],
            [
                "Alon",
                "Dana",
                ""
            ],
            [
                "Herrmann",
                "Charles",
                ""
            ],
            [
                "van Steenkiste",
                "Sjoerd",
                ""
            ],
            [
                "Krishna",
                "Ranjay",
                ""
            ],
            [
                "Rashtchian",
                "Cyrus",
                ""
            ]
        ],
        "categories": "cs.CV cs.AI cs.CL",
        "comments": null,
        "doi": null,
        "id": "2311.17946",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Deqing Fu",
        "title": "DreamSync: Aligning Text-to-Image Generation with Image Understanding\n  Feedback",
        "update_date": "2023-12-01",
        "versions": [
            {
                "created": "Wed, 29 Nov 2023 03:42:16 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Recent advances in natural language processing, primarily propelled by Large\nLanguage Models (LLMs), have showcased their remarkable capabilities grounded\nin in-context learning. A promising avenue for guiding LLMs in intricate\nreasoning tasks involves the utilization of intermediate reasoning steps within\nthe Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies\nin the effective selection of exemplars for facilitating in-context learning.\nIn this study, we introduce a framework that leverages Dual Queries and\nLow-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars\nfor in-context learning. Dual Queries first query LLM to obtain LLM-generated\nknowledge such as CoT, then query the retriever to obtain the final exemplars\nvia both question and the knowledge. Moreover, for the second query, LoRe\nemploys dimensionality reduction techniques to refine exemplar selection,\nensuring close alignment with the input question's knowledge. Through extensive\nexperiments, we demonstrate that DQ-LoRe significantly outperforms prior\nstate-of-the-art methods in the automatic selection of exemplars for GPT-4,\nenhancing performance from 92.5% to 94.2%. Our comprehensive analysis further\nreveals that DQ-LoRe consistently outperforms retrieval-based approaches in\nterms of both performance and adaptability, especially in scenarios\ncharacterized by distribution shifts. DQ-LoRe pushes the boundary of in-context\nlearning and opens up new avenues for addressing complex reasoning challenges.\nOur code is released at\nhttps://github.com/AI4fun/DQ-LoRe}{https://github.com/AI4fun/DQ-LoRe.\n",
        "authors": "Jing Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze\n  Xie, Zhicheng Yang, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang,\n  Chengming Li, Xiaodan Liang",
        "authors_parsed": [
            [
                "Xiong",
                "Jing",
                ""
            ],
            [
                "Li",
                "Zixuan",
                ""
            ],
            [
                "Zheng",
                "Chuanyang",
                ""
            ],
            [
                "Guo",
                "Zhijiang",
                ""
            ],
            [
                "Yin",
                "Yichun",
                ""
            ],
            [
                "Xie",
                "Enze",
                ""
            ],
            [
                "Yang",
                "Zhicheng",
                ""
            ],
            [
                "Cao",
                "Qingxing",
                ""
            ],
            [
                "Wang",
                "Haiming",
                ""
            ],
            [
                "Han",
                "Xiongwei",
                ""
            ],
            [
                "Tang",
                "Jing",
                ""
            ],
            [
                "Li",
                "Chengming",
                ""
            ],
            [
                "Liang",
                "Xiaodan",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "Accepted in ICLR 2024",
        "doi": null,
        "id": "2310.02954",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Jing Xiong",
        "title": "DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for\n  In-Context Learning",
        "update_date": "2024-03-05",
        "versions": [
            {
                "created": "Wed, 4 Oct 2023 16:44:37 GMT",
                "version": "v1"
            },
            {
                "created": "Thu, 5 Oct 2023 16:24:58 GMT",
                "version": "v2"
            },
            {
                "created": "Thu, 19 Oct 2023 02:16:20 GMT",
                "version": "v3"
            },
            {
                "created": "Fri, 20 Oct 2023 07:50:50 GMT",
                "version": "v4"
            },
            {
                "created": "Sat, 2 Mar 2024 14:38:24 GMT",
                "version": "v5"
            }
        ]
    },
    {
        "abstract": "  Multimodal Federated Learning (MMFL) utilizes multiple modalities in each\nclient to build a more powerful Federated Learning (FL) model than its unimodal\ncounterpart. However, the impact of missing modality in different clients, also\ncalled modality incongruity, has been greatly overlooked. This paper, for the\nfirst time, analyses the impact of modality incongruity and reveals its\nconnection with data heterogeneity across participating clients. We\nparticularly inspect whether incongruent MMFL with unimodal and multimodal\nclients is more beneficial than unimodal FL. Furthermore, we examine three\npotential routes of addressing this issue. Firstly, we study the effectiveness\nof various self-attention mechanisms towards incongruity-agnostic information\nfusion in MMFL. Secondly, we introduce a modality imputation network (MIN)\npre-trained in a multimodal client for modality translation in unimodal clients\nand investigate its potential towards mitigating the missing modality problem.\nThirdly, we assess the capability of client-level and server-level\nregularization techniques towards mitigating modality incongruity effects.\nExperiments are conducted under several MMFL settings on two publicly available\nreal-world datasets, MIMIC-CXR and Open-I, with Chest X-Ray and radiology\nreports.\n",
        "authors": "Pramit Saha, Divyanshu Mishra, Felix Wagner, Konstantinos Kamnitsas,\n  J. Alison Noble",
        "authors_parsed": [
            [
                "Saha",
                "Pramit",
                ""
            ],
            [
                "Mishra",
                "Divyanshu",
                ""
            ],
            [
                "Wagner",
                "Felix",
                ""
            ],
            [
                "Kamnitsas",
                "Konstantinos",
                ""
            ],
            [
                "Noble",
                "J. Alison",
                ""
            ]
        ],
        "categories": "cs.LG cs.AI cs.CL cs.CV",
        "comments": "42 pages",
        "doi": null,
        "id": "2402.05294",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Pramit Saha",
        "title": "Examining Modality Incongruity in Multimodal Federated Learning for\n  Medical Vision and Language-based Disease Detection",
        "update_date": "2024-02-09",
        "versions": [
            {
                "created": "Wed, 7 Feb 2024 22:16:53 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Text evaluation has historically posed significant challenges, often\ndemanding substantial labor and time cost. With the emergence of large language\nmodels (LLMs), researchers have explored LLMs' potential as alternatives for\nhuman evaluation. While these single-agent-based approaches show promise,\nexperimental results suggest that further advancements are needed to bridge the\ngap between their current effectiveness and human-level evaluation quality.\nRecognizing that best practices of human evaluation processes often involve\nmultiple human annotators collaborating in the evaluation, we resort to a\nmulti-agent debate framework, moving beyond single-agent prompting strategies.\nThe multi-agent-based approach enables a group of LLMs to synergize with an\narray of intelligent counterparts, harnessing their distinct capabilities and\nexpertise to enhance efficiency and effectiveness in handling intricate tasks.\nIn this paper, we construct a multi-agent referee team called ChatEval to\nautonomously discuss and evaluate the quality of generated responses from\ndifferent models on open-ended questions and traditional natural language\ngeneration (NLG) tasks. Our analysis shows that ChatEval transcends mere\ntextual scoring, offering a human-mimicking evaluation process for reliable\nassessments. Our code is available at https://github.com/chanchimin/ChatEval.\n",
        "authors": "Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang\n  Zhang, Jie Fu, Zhiyuan Liu",
        "authors_parsed": [
            [
                "Chan",
                "Chi-Min",
                ""
            ],
            [
                "Chen",
                "Weize",
                ""
            ],
            [
                "Su",
                "Yusheng",
                ""
            ],
            [
                "Yu",
                "Jianxuan",
                ""
            ],
            [
                "Xue",
                "Wei",
                ""
            ],
            [
                "Zhang",
                "Shanghang",
                ""
            ],
            [
                "Fu",
                "Jie",
                ""
            ],
            [
                "Liu",
                "Zhiyuan",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": null,
        "doi": null,
        "id": "2308.07201",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Chi-Min Chan",
        "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
        "update_date": "2023-08-15",
        "versions": [
            {
                "created": "Mon, 14 Aug 2023 15:13:04 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Artificial intelligence's (AI) progress holds great promise in tackling\npressing societal concerns such as health and climate. Large Language Models\n(LLM) and the derived chatbots, like ChatGPT, have highly improved the natural\nlanguage processing capabilities of AI systems allowing them to process an\nunprecedented amount of unstructured data. However, the ensuing excitement has\nled to negative sentiments, even as AI methods demonstrate remarkable\ncontributions (e.g. in health and genetics). A key factor contributing to this\nsentiment is the misleading perception that LLMs can effortlessly provide\nsolutions across domains, ignoring their limitations such as hallucinations and\nreasoning constraints. Acknowledging AI fallibility is crucial to address the\nimpact of dogmatic overconfidence in possibly erroneous suggestions generated\nby LLMs. At the same time, it can reduce fear and other negative attitudes\ntoward AI. This necessitates comprehensive AI literacy interventions that\neducate the public about LLM constraints and effective usage techniques, i.e\nprompting strategies. With this aim, a pilot educational intervention was\nperformed in a high school with 21 students. It involved presenting high-level\nconcepts about intelligence, AI, and LLMs, followed by practical exercises\ninvolving ChatGPT in creating natural educational conversations and applying\nestablished prompting strategies. Encouraging preliminary results emerged,\nincluding high appreciation of the activity, improved interaction quality with\nthe LLM, reduced negative AI sentiments, and a better grasp of limitations,\nspecifically unreliability, limited understanding of commands leading to\nunsatisfactory responses, and limited presentation flexibility. Our aim is to\nexplore AI acceptance factors and refine this approach for more controlled\nfuture studies.\n",
        "authors": "Emily Theophilou, Cansu Koyuturk, Mona Yavari, Sathya Bursic, Gregor\n  Donabauer, Alessia Telari, Alessia Testa, Raffaele Boiano, Davinia\n  Hernandez-Leo, Martin Ruskov, Davide Taibi, Alessandro Gabbiadini, Dimitri\n  Ognibene",
        "authors_parsed": [
            [
                "Theophilou",
                "Emily",
                ""
            ],
            [
                "Koyuturk",
                "Cansu",
                ""
            ],
            [
                "Yavari",
                "Mona",
                ""
            ],
            [
                "Bursic",
                "Sathya",
                ""
            ],
            [
                "Donabauer",
                "Gregor",
                ""
            ],
            [
                "Telari",
                "Alessia",
                ""
            ],
            [
                "Testa",
                "Alessia",
                ""
            ],
            [
                "Boiano",
                "Raffaele",
                ""
            ],
            [
                "Hernandez-Leo",
                "Davinia",
                ""
            ],
            [
                "Ruskov",
                "Martin",
                ""
            ],
            [
                "Taibi",
                "Davide",
                ""
            ],
            [
                "Gabbiadini",
                "Alessandro",
                ""
            ],
            [
                "Ognibene",
                "Dimitri",
                ""
            ]
        ],
        "categories": "cs.HC cs.AI cs.CL",
        "comments": "Accepted for AIXIA 2023 22nd International Conference of the Italian\n  Association for Artificial Intelligence 6 - 9 Nov, 2023, Rome, Italy",
        "doi": null,
        "id": "2307.01540",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Dimitri Ognibene",
        "title": "Learning to Prompt in the Classroom to Understand AI Limits: A pilot\n  study",
        "update_date": "2023-09-04",
        "versions": [
            {
                "created": "Tue, 4 Jul 2023 07:51:37 GMT",
                "version": "v1"
            },
            {
                "created": "Fri, 1 Sep 2023 15:31:21 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  This paper presents a method for selecting appropriate synthetic speech\nsamples from a given large text-to-speech (TTS) dataset as supplementary\ntraining data for an automatic speech recognition (ASR) model. We trained a\nneural network, which can be optimised using cross-entropy loss or Arcface\nloss, to measure the similarity of a synthetic data to real speech. We found\nthat incorporating synthetic samples with considerable dissimilarity to real\nspeech, owing in part to lexical differences, into ASR training is crucial for\nboosting recognition performance. Experimental results on Librispeech test sets\nindicate that, in order to maintain the same speech recognition accuracy as\nwhen using all TTS data, our proposed solution can reduce the size of the TTS\ndata down below its $30\\,\\%$, which is superior to several baseline methods.\n",
        "authors": "Shuo Liu, Leda Sar{\\i}, Chunyang Wu, Gil Keren, Yuan Shangguan, Jay\n  Mahadeokar, Ozlem Kalinli",
        "authors_parsed": [
            [
                "Liu",
                "Shuo",
                ""
            ],
            [
                "Sar\u0131",
                "Leda",
                ""
            ],
            [
                "Wu",
                "Chunyang",
                ""
            ],
            [
                "Keren",
                "Gil",
                ""
            ],
            [
                "Shangguan",
                "Yuan",
                ""
            ],
            [
                "Mahadeokar",
                "Jay",
                ""
            ],
            [
                "Kalinli",
                "Ozlem",
                ""
            ]
        ],
        "categories": "eess.AS cs.CL cs.SD",
        "comments": null,
        "doi": null,
        "id": "2306.00998",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Shuo Liu",
        "title": "Towards Selection of Text-to-speech Data to Augment ASR Training",
        "update_date": "2023-06-05",
        "versions": [
            {
                "created": "Tue, 30 May 2023 17:24:28 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Extracting commonsense knowledge from a large language model (LLM) offers a\npath to designing intelligent robots. Existing approaches that leverage LLMs\nfor planning are unable to recover when an action fails and often resort to\nretrying failed actions, without resolving the error's underlying cause. We\npropose a novel approach (CAPE) that attempts to propose corrective actions to\nresolve precondition errors during planning. CAPE improves the quality of\ngenerated plans by leveraging few-shot reasoning from action preconditions. Our\napproach enables embodied agents to execute more tasks than baseline methods\nwhile ensuring semantic correctness and minimizing re-prompting. In\nVirtualHome, CAPE generates executable plans while improving a human-annotated\nplan correctness metric from 28.89% to 49.63% over SayCan. Our improvements\ntransfer to a Boston Dynamics Spot robot initialized with a set of skills\n(specified in language) and associated preconditions, where CAPE improves the\ncorrectness metric of the executed task plans by 76.49% compared to SayCan. Our\napproach enables the robot to follow natural language commands and robustly\nrecover from failures, which baseline approaches largely cannot resolve or\naddress inefficiently.\n",
        "authors": "Shreyas Sundara Raman, Vanya Cohen, Ifrah Idrees, Eric Rosen, Ray\n  Mooney, Stefanie Tellex and David Paulius",
        "authors_parsed": [
            [
                "Raman",
                "Shreyas Sundara",
                ""
            ],
            [
                "Cohen",
                "Vanya",
                ""
            ],
            [
                "Idrees",
                "Ifrah",
                ""
            ],
            [
                "Rosen",
                "Eric",
                ""
            ],
            [
                "Mooney",
                "Ray",
                ""
            ],
            [
                "Tellex",
                "Stefanie",
                ""
            ],
            [
                "Paulius",
                "David",
                ""
            ]
        ],
        "categories": "cs.AI cs.CL cs.LG cs.RO",
        "comments": "17 pages, 6 figures, accepted at ICRA 2024",
        "doi": null,
        "id": "2211.09935",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Shreyas Sundara Raman",
        "title": "CAPE: Corrective Actions from Precondition Errors using Large Language\n  Models",
        "update_date": "2024-03-12",
        "versions": [
            {
                "created": "Thu, 17 Nov 2022 23:14:51 GMT",
                "version": "v1"
            },
            {
                "created": "Mon, 23 Oct 2023 03:56:16 GMT",
                "version": "v2"
            },
            {
                "created": "Sat, 9 Mar 2024 13:53:47 GMT",
                "version": "v3"
            }
        ]
    },
    {
        "abstract": "  The increased interest in diffusion models has opened up opportunities for\nadvancements in generative text modeling. These models can produce impressive\nimages when given a well-crafted prompt, but creating a powerful or meaningful\nprompt can be hit-or-miss. To address this, we have created a large-scale\ndataset that is derived and synthesized from real prompts and indexed with\npopular image-text datasets such as MS-COCO and Flickr. We have also\nimplemented stages that gradually reduce context and increase complexity, which\nwill further enhance the output due to the complex annotations created. The\ndataset, called MTTN, includes over 2.4 million sentences divided into 5\nstages, resulting in a total of over 12 million pairs, and a vocabulary of over\n300,000 unique words, providing ample variation. The original 2.4 million pairs\nare designed to reflect the way language is used on the internet globally,\nmaking the dataset more robust for any model trained on it.\n",
        "authors": "Archan Ghosh, Debgandhar Ghosh, Madhurima Maji, Suchinta Chanda,\n  Kalporup Goswami",
        "authors_parsed": [
            [
                "Ghosh",
                "Archan",
                ""
            ],
            [
                "Ghosh",
                "Debgandhar",
                ""
            ],
            [
                "Maji",
                "Madhurima",
                ""
            ],
            [
                "Chanda",
                "Suchinta",
                ""
            ],
            [
                "Goswami",
                "Kalporup",
                ""
            ]
        ],
        "categories": "cs.CL cs.LG",
        "comments": null,
        "doi": null,
        "id": "2301.10172",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "report-no": null,
        "submitter": "Archan Ghosh",
        "title": "MTTN: Multi-Pair Text to Text Narratives for Prompt Generation",
        "update_date": "2023-01-31",
        "versions": [
            {
                "created": "Sat, 21 Jan 2023 06:55:44 GMT",
                "version": "v1"
            },
            {
                "created": "Sun, 29 Jan 2023 18:03:44 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  The rapid advancement of Large Language Models (LLMs) has demonstrated their\nvast potential across various domains, attributed to their extensive\npretraining knowledge and exceptional generalizability. However, LLMs often\nencounter challenges in generating harmful content when faced with problematic\nprompts. To address this problem, existing work attempted to implement a\ngradient ascent based approach to prevent LLMs from producing harmful output.\nWhile these methods can be effective, they frequently impact the model utility\nin responding to normal prompts. To address this gap, we introduce Selective\nKnowledge negation Unlearning (SKU), a novel unlearning framework for LLMs,\ndesigned to eliminate harmful knowledge while preserving utility on normal\nprompts. Specifically, SKU is consisted of two stages: harmful knowledge\nacquisition stage and knowledge negation stage. The first stage aims to\nidentify and acquire harmful knowledge within the model, whereas the second is\ndedicated to remove this knowledge. SKU selectively isolates and removes\nharmful knowledge in model parameters, ensuring the model's performance remains\nrobust on normal prompts. Our experiments conducted across various LLM\narchitectures demonstrate that SKU identifies a good balance point between\nremoving harmful information and preserving utility.\n",
        "authors": "Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, Meng Jiang",
        "authors_parsed": [
            [
                "Liu",
                "Zheyuan",
                ""
            ],
            [
                "Dou",
                "Guangyao",
                ""
            ],
            [
                "Tan",
                "Zhaoxuan",
                ""
            ],
            [
                "Tian",
                "Yijun",
                ""
            ],
            [
                "Jiang",
                "Meng",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "13 pages in total",
        "doi": null,
        "id": "2402.10058",
        "journal-ref": null,
        "license": "http://creativecommons.org/publicdomain/zero/1.0/",
        "report-no": null,
        "submitter": "Zheyuan Liu",
        "title": "Towards Safer Large Language Models through Machine Unlearning",
        "update_date": "2024-02-16",
        "versions": [
            {
                "created": "Thu, 15 Feb 2024 16:28:34 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Large vision-language models (LVLMs) have shown remarkable abilities in\nunderstanding visual information with human languages. However, LVLMs still\nsuffer from object hallucination, which is the problem of generating\ndescriptions that include objects that do not actually exist in the images.\nThis can negatively impact many vision-language tasks, such as visual\nsummarization and reasoning. To address this issue, we propose a simple yet\npowerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify\nobject hallucination in LVLMs by reconstructing less hallucinatory\ndescriptions. LURE is grounded in a rigorous statistical analysis of the key\nfactors underlying object hallucination, including co-occurrence (the frequent\nappearance of certain objects alongside others in images), uncertainty (objects\nwith higher uncertainty during LVLM decoding), and object position\n(hallucination often appears in the later part of the generated text). LURE can\nalso be seamlessly integrated with any LVLMs. We evaluate LURE on six\nopen-source LVLMs, achieving a 23% improvement in general object hallucination\nevaluation metrics over the previous best approach. In both GPT and human\nevaluations, LURE consistently ranks at the top. Our data and code are\navailable at https://github.com/YiyangZhou/LURE.\n",
        "authors": "Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng,\n  Chelsea Finn, Mohit Bansal, Huaxiu Yao",
        "authors_parsed": [
            [
                "Zhou",
                "Yiyang",
                ""
            ],
            [
                "Cui",
                "Chenhang",
                ""
            ],
            [
                "Yoon",
                "Jaehong",
                ""
            ],
            [
                "Zhang",
                "Linjun",
                ""
            ],
            [
                "Deng",
                "Zhun",
                ""
            ],
            [
                "Finn",
                "Chelsea",
                ""
            ],
            [
                "Bansal",
                "Mohit",
                ""
            ],
            [
                "Yao",
                "Huaxiu",
                ""
            ]
        ],
        "categories": "cs.LG cs.CL cs.CV",
        "comments": "Accepted by ICLR 2024",
        "doi": null,
        "id": "2310.00754",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Huaxiu Yao",
        "title": "Analyzing and Mitigating Object Hallucination in Large Vision-Language\n  Models",
        "update_date": "2024-03-19",
        "versions": [
            {
                "created": "Sun, 1 Oct 2023 18:10:53 GMT",
                "version": "v1"
            },
            {
                "created": "Sat, 16 Mar 2024 19:28:08 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  As large language models (LLMs) are adopted as a fundamental component of\nlanguage technologies, it is crucial to accurately characterize their\nperformance. Because choices in prompt design can strongly influence model\nbehavior, this design process is critical in effectively using any modern\npre-trained generative language model. In this work, we focus on LLM\nsensitivity to a quintessential class of meaning-preserving design choices:\nprompt formatting. We find that several widely used open-source LLMs are\nextremely sensitive to subtle changes in prompt formatting in few-shot\nsettings, with performance differences of up to 76 accuracy points when\nevaluated using LLaMA-2-13B. Sensitivity remains even when increasing model\nsize, the number of few-shot examples, or performing instruction tuning. Our\nanalysis suggests that work evaluating LLMs with prompting-based methods would\nbenefit from reporting a range of performance across plausible prompt formats,\ninstead of the currently-standard practice of reporting performance on a single\nformat. We also show that format performance only weakly correlates between\nmodels, which puts into question the methodological validity of comparing\nmodels with an arbitrarily chosen, fixed prompt format. To facilitate\nsystematic analysis we propose FormatSpread, an algorithm that rapidly\nevaluates a sampled set of plausible prompt formats for a given task, and\nreports the interval of expected performance without accessing model weights.\nFurthermore, we present a suite of analyses that characterize the nature of\nthis sensitivity, including exploring the influence of particular atomic\nperturbations and the internal representation of particular formats.\n",
        "authors": "Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr",
        "authors_parsed": [
            [
                "Sclar",
                "Melanie",
                ""
            ],
            [
                "Choi",
                "Yejin",
                ""
            ],
            [
                "Tsvetkov",
                "Yulia",
                ""
            ],
            [
                "Suhr",
                "Alane",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI cs.LG",
        "comments": null,
        "doi": null,
        "id": "2310.11324",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Melanie Sclar",
        "title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt\n  Design or: How I learned to start worrying about prompt formatting",
        "update_date": "2023-10-18",
        "versions": [
            {
                "created": "Tue, 17 Oct 2023 15:03:30 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Large Language Models (LLMs) have demonstrated remarkable problem-solving and\nbasic mathematics abilities. However, their efficacy is highly contingent on\nthe formulation of the prompt. This study endeavors to quantify the influence\nof incorporating \"positive thinking\" into the system message of the prompt,\nthen compare that to systematic prompt optimization. We assess the performance\nof 60 combinations of system message snippets, tested with and without Chain of\nThought prompting, across three models with parameters ranging from 7 to 70\nbillion on the GSM8K dataset. Our findings reveal that results do not\nuniversally generalize across models. In most instances, the inclusion of\n\"positive thinking\" prompts positively affected model performance. Notably,\nhowever, Llama2-70B exhibited an exception when not utilizing Chain of Thought,\nas the optimal system message was found to be none at all. Given the\ncombinatorial complexity, and thus computation time, of experimenting with\nhand-tuning prompts for large black-box models, we then compared the\nperformance of the best \"positive thinking\" prompt against the output of\nsystematic prompt optimization. We show that employing an automated prompt\noptimizer emerges as the most effective method for enhancing performance, even\nwhen working with smaller open-source models. Additionally, our findings reveal\nthat the highest-scoring, automatically-optimized prompt exhibits a degree of\npeculiarity far beyond expectations.\n",
        "authors": "Rick Battle and Teja Gollapudi",
        "authors_parsed": [
            [
                "Battle",
                "Rick",
                ""
            ],
            [
                "Gollapudi",
                "Teja",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI cs.LG",
        "comments": null,
        "doi": null,
        "id": "2402.10949",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Rick Battle",
        "title": "The Unreasonable Effectiveness of Eccentric Automatic Prompts",
        "update_date": "2024-02-21",
        "versions": [
            {
                "created": "Fri, 9 Feb 2024 22:48:45 GMT",
                "version": "v1"
            },
            {
                "created": "Tue, 20 Feb 2024 15:03:00 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Augmenting large language models (LLMs) with external tools has emerged as a\npromising approach to extending the capability of LLMs. Although some works\nemploy open-source LLMs for the tool learning task, most of them are trained in\na controlled environment in which LLMs only learn to execute the human-provided\ntools. However, selecting proper tools from the large toolset is also a crucial\nability for the tool learning model to be applied in real-world applications.\nExisting methods usually directly employ self-instruction methods to train the\nmodel, which ignores differences in tool complexity. In this paper, we propose\nthe Confucius, a novel tool learning framework to train LLM to use complicated\ntools in real-world scenarios, which contains two main phases: (1) We first\npropose a multi-stage learning method to teach the LLM to use various tools\nfrom an easy-to-difficult curriculum; (2) thenceforth, we propose the Iterative\nSelf-instruct from Introspective Feedback (ISIF) to dynamically construct the\ndataset to improve the ability to use the complicated tool. Extensive\nexperiments conducted on both controlled and real-world settings demonstrate\nthe superiority of our tool learning framework in the real-world application\nscenarios compared to both tuning-free (e.g. ChatGPT, Claude) and tuning-based\nbaselines (e.g. GPT4Tools).\n",
        "authors": "Shen Gao, Zhengliang Shi, Minghang Zhu, Bowen Fang, Xin Xin, Pengjie\n  Ren, Zhumin Chen, Jun Ma, Zhaochun Ren",
        "authors_parsed": [
            [
                "Gao",
                "Shen",
                ""
            ],
            [
                "Shi",
                "Zhengliang",
                ""
            ],
            [
                "Zhu",
                "Minghang",
                ""
            ],
            [
                "Fang",
                "Bowen",
                ""
            ],
            [
                "Xin",
                "Xin",
                ""
            ],
            [
                "Ren",
                "Pengjie",
                ""
            ],
            [
                "Chen",
                "Zhumin",
                ""
            ],
            [
                "Ma",
                "Jun",
                ""
            ],
            [
                "Ren",
                "Zhaochun",
                ""
            ]
        ],
        "categories": "cs.AI cs.CL",
        "comments": "Accepted by AAAI 2024",
        "doi": null,
        "id": "2308.14034",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Shen Gao",
        "title": "Confucius: Iterative Tool Learning from Introspection Feedback by\n  Easy-to-Difficult Curriculum",
        "update_date": "2023-12-22",
        "versions": [
            {
                "created": "Sun, 27 Aug 2023 07:53:00 GMT",
                "version": "v1"
            },
            {
                "created": "Thu, 21 Dec 2023 07:30:31 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Self-supervised pre-training of language models usually consists in\npredicting probability distributions over extensive token vocabularies. In this\nstudy, we propose an innovative method that shifts away from probability\nprediction and instead focuses on reconstructing input embeddings in a\ncontrastive fashion via Constrastive Weight Tying (CWT). We apply this approach\nto pretrain Headless Language Models in both monolingual and multilingual\ncontexts. Our method offers practical advantages, substantially reducing\ntraining computational requirements by up to 20 times, while simultaneously\nenhancing downstream performance and data efficiency. We observe a significant\n+1.6 GLUE score increase and a notable +2.7 LAMBADA accuracy improvement\ncompared to classical LMs within similar compute budgets.\n",
        "authors": "Nathan Godey, \\'Eric de la Clergerie, Beno\\^it Sagot",
        "authors_parsed": [
            [
                "Godey",
                "Nathan",
                ""
            ],
            [
                "de la Clergerie",
                "\u00c9ric",
                ""
            ],
            [
                "Sagot",
                "Beno\u00eet",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": null,
        "doi": null,
        "id": "2309.08351",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "report-no": null,
        "submitter": "Nathan Godey",
        "title": "Headless Language Models: Learning without Predicting with Contrastive\n  Weight Tying",
        "update_date": "2023-09-18",
        "versions": [
            {
                "created": "Fri, 15 Sep 2023 12:20:00 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Conversational recommender systems (CRS) aim to recommend suitable items to\nusers through natural language conversations. For developing effective CRSs, a\nmajor technical issue is how to accurately infer user preference from very\nlimited conversation context. To address issue, a promising solution is to\nincorporate external data for enriching the context information. However, prior\nstudies mainly focus on designing fusion models tailored for some specific type\nof external data, which is not general to model and utilize multi-type external\ndata.\n  To effectively leverage multi-type external data, we propose a novel\ncoarse-to-fine contrastive learning framework to improve data semantic fusion\nfor CRS. In our approach, we first extract and represent multi-grained semantic\nunits from different data signals, and then align the associated multi-type\nsemantic units in a coarse-to-fine way. To implement this framework, we design\nboth coarse-grained and fine-grained procedures for modeling user preference,\nwhere the former focuses on more general, coarse-grained semantic fusion and\nthe latter focuses on more specific, fine-grained semantic fusion. Such an\napproach can be extended to incorporate more kinds of external data. Extensive\nexperiments on two public CRS datasets have demonstrated the effectiveness of\nour approach in both recommendation and conversation tasks.\n",
        "authors": "Yuanhang Zhou, Kun Zhou, Wayne Xin Zhao, Cheng Wang, Peng Jiang, He Hu",
        "authors_parsed": [
            [
                "Zhou",
                "Yuanhang",
                ""
            ],
            [
                "Zhou",
                "Kun",
                ""
            ],
            [
                "Zhao",
                "Wayne Xin",
                ""
            ],
            [
                "Wang",
                "Cheng",
                ""
            ],
            [
                "Jiang",
                "Peng",
                ""
            ],
            [
                "Hu",
                "He",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI cs.IR",
        "comments": null,
        "doi": null,
        "id": "2201.02732",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Yuanhang Zhou",
        "title": "C2-CRS: Coarse-to-Fine Contrastive Learning for Conversational\n  Recommender System",
        "update_date": "2023-05-31",
        "versions": [
            {
                "created": "Tue, 4 Jan 2022 11:39:41 GMT",
                "version": "v1"
            },
            {
                "created": "Thu, 31 Mar 2022 03:46:07 GMT",
                "version": "v2"
            },
            {
                "created": "Tue, 30 May 2023 06:37:40 GMT",
                "version": "v3"
            }
        ]
    },
    {
        "abstract": "  Large language models (LLMs), like ChatGPT, have shown some human-like\ncognitive abilities. For comparing these abilities of different models, several\nbenchmarks (i.e. sets of standard test questions) from different fields (e.g.,\nLiterature, Biology and Psychology) are often adopted and the test results\nunder traditional metrics such as accuracy, recall and F1, are reported.\nHowever, such way for evaluating LLMs can be inefficient and inaccurate from\nthe cognitive science perspective. Inspired by Computerized Adaptive Testing\n(CAT) used in psychometrics, we propose an adaptive testing framework for LLM\nevaluation. Rather than using a standard test set and simply reporting\naccuracy, this approach dynamically adjusts the characteristics of the test\nquestions, such as difficulty, based on the model's performance. This allows\nfor a more accurate estimation of the model's abilities, using fewer questions.\nMore importantly, it allows LLMs to be compared with humans easily, which is\nessential for NLP models that aim for human-level ability. Our diagnostic\nreports have found that ChatGPT often behaves like a ``careless student'',\nprone to slip and occasionally guessing the questions. We conduct a\nfine-grained diagnosis and rank the latest 6 instruction-tuned LLMs from three\naspects of Subject Knowledge, Mathematical Reasoning, and Programming, where\nGPT4 can outperform other models significantly and reach the cognitive ability\nof middle-level students. Different tests for different models using efficient\nadaptive testing -- we believe this has the potential to become a new norm in\nevaluating large language models.\n",
        "authors": "Yan Zhuang, Qi Liu, Yuting Ning, Weizhe Huang, Rui Lv, Zhenya Huang,\n  Guanhao Zhao, Zheng Zhang, Qingyang Mao, Shijin Wang, Enhong Chen",
        "authors_parsed": [
            [
                "Zhuang",
                "Yan",
                ""
            ],
            [
                "Liu",
                "Qi",
                ""
            ],
            [
                "Ning",
                "Yuting",
                ""
            ],
            [
                "Huang",
                "Weizhe",
                ""
            ],
            [
                "Lv",
                "Rui",
                ""
            ],
            [
                "Huang",
                "Zhenya",
                ""
            ],
            [
                "Zhao",
                "Guanhao",
                ""
            ],
            [
                "Zhang",
                "Zheng",
                ""
            ],
            [
                "Mao",
                "Qingyang",
                ""
            ],
            [
                "Wang",
                "Shijin",
                ""
            ],
            [
                "Chen",
                "Enhong",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": null,
        "doi": null,
        "id": "2306.10512",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Yan Zhuang",
        "title": "Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing\n  Perspective",
        "update_date": "2023-10-31",
        "versions": [
            {
                "created": "Sun, 18 Jun 2023 09:54:33 GMT",
                "version": "v1"
            },
            {
                "created": "Sat, 28 Oct 2023 13:02:24 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Change captioning is to describe the semantic change between a pair of\nsimilar images in natural language. It is more challenging than general image\ncaptioning, because it requires capturing fine-grained change information while\nbeing immune to irrelevant viewpoint changes, and solving syntax ambiguity in\nchange descriptions. In this paper, we propose a neighborhood contrastive\ntransformer to improve the model's perceiving ability for various changes under\ndifferent scenes and cognition ability for complex syntax structure.\nConcretely, we first design a neighboring feature aggregating to integrate\nneighboring context into each feature, which helps quickly locate the\ninconspicuous changes under the guidance of conspicuous referents. Then, we\ndevise a common feature distilling to compare two images at neighborhood level\nand extract common properties from each image, so as to learn effective\ncontrastive information between them. Finally, we introduce the explicit\ndependencies between words to calibrate the transformer decoder, which helps\nbetter understand complex syntax structure during training. Extensive\nexperimental results demonstrate that the proposed method achieves the\nstate-of-the-art performance on three public datasets with different change\nscenarios. The code is available at https://github.com/tuyunbin/NCT.\n",
        "authors": "Yunbin Tu, Liang Li, Li Su, Ke Lu, Qingming Huang",
        "authors_parsed": [
            [
                "Tu",
                "Yunbin",
                ""
            ],
            [
                "Li",
                "Liang",
                ""
            ],
            [
                "Su",
                "Li",
                ""
            ],
            [
                "Lu",
                "Ke",
                ""
            ],
            [
                "Huang",
                "Qingming",
                ""
            ]
        ],
        "categories": "cs.CV cs.CL cs.MM",
        "comments": "Accepted by IEEE TMM",
        "doi": null,
        "id": "2303.03171",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Yunbin Tu",
        "title": "Neighborhood Contrastive Transformer for Change Captioning",
        "update_date": "2023-03-07",
        "versions": [
            {
                "created": "Mon, 6 Mar 2023 14:39:54 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Neural networks are capable of translating between languages -- in some cases\neven between two languages where there is little or no access to parallel\ntranslations, in what is known as Unsupervised Machine Translation (UMT). Given\nthis progress, it is intriguing to ask whether machine learning tools can\nultimately enable understanding animal communication, particularly that of\nhighly intelligent animals. We propose a theoretical framework for analyzing\nUMT when no parallel translations are available and when it cannot be assumed\nthat the source and target corpora address related subject domains or posses\nsimilar linguistic structure. We exemplify this theory with two stylized models\nof language, for which our framework provides bounds on necessary sample\ncomplexity; the bounds are formally proven and experimentally verified on\nsynthetic data. These bounds show that the error rates are inversely related to\nthe language complexity and amount of common ground. This suggests that\nunsupervised translation of animal communication may be feasible if the\ncommunication system is sufficiently complex.\n",
        "authors": "Shafi Goldwasser, David F. Gruber, Adam Tauman Kalai, Orr Paradise",
        "authors_parsed": [
            [
                "Goldwasser",
                "Shafi",
                ""
            ],
            [
                "Gruber",
                "David F.",
                ""
            ],
            [
                "Kalai",
                "Adam Tauman",
                ""
            ],
            [
                "Paradise",
                "Orr",
                ""
            ]
        ],
        "categories": "cs.CL cs.LG",
        "comments": null,
        "doi": null,
        "id": "2211.11081",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Orr Paradise",
        "title": "A Theory of Unsupervised Translation Motivated by Understanding Animal\n  Communication",
        "update_date": "2023-11-07",
        "versions": [
            {
                "created": "Sun, 20 Nov 2022 20:55:38 GMT",
                "version": "v1"
            },
            {
                "created": "Fri, 3 Nov 2023 18:15:59 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Tasks involving text generation based on multiple input texts, such as\nmulti-document summarization, long-form question answering and contemporary\ndialogue applications, challenge models for their ability to properly\nconsolidate partly-overlapping multi-text information. However, these tasks\nentangle the consolidation phase with the often subjective and ill-defined\ncontent selection requirement, impeding proper assessment of models'\nconsolidation capabilities. In this paper, we suggest revisiting the sentence\nunion generation task as an effective well-defined testbed for assessing text\nconsolidation capabilities, decoupling the consolidation challenge from\nsubjective content selection. To support research on this task, we present\nrefined annotation methodology and tools for crowdsourcing sentence union,\ncreate the largest union dataset to date and provide an analysis of its rich\ncoverage of various consolidation aspects. We then propose a comprehensive\nevaluation protocol for union generation, including both human and automatic\nevaluation. Finally, as baselines, we evaluate state-of-the-art language models\non the task, along with a detailed analysis of their capacity to address\nmulti-text consolidation challenges and their limitations.\n",
        "authors": "Eran Hirsch, Valentina Pyatkin, Ruben Wolhandler, Avi Caciularu, Asi\n  Shefer, Ido Dagan",
        "authors_parsed": [
            [
                "Hirsch",
                "Eran",
                ""
            ],
            [
                "Pyatkin",
                "Valentina",
                ""
            ],
            [
                "Wolhandler",
                "Ruben",
                ""
            ],
            [
                "Caciularu",
                "Avi",
                ""
            ],
            [
                "Shefer",
                "Asi",
                ""
            ],
            [
                "Dagan",
                "Ido",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "Findings of the Association for Computational Linguistics (ACL 2023)",
        "doi": null,
        "id": "2305.15605",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Eran Hirsch",
        "title": "Revisiting Sentence Union Generation as a Testbed for Text Consolidation",
        "update_date": "2023-05-26",
        "versions": [
            {
                "created": "Wed, 24 May 2023 22:34:01 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Clarification requests are a mechanism to help solve communication problems,\ne.g. due to ambiguity or underspecification, in instruction-following\ninteractions. Despite their importance, even skilful models struggle with\nproducing or interpreting such repair acts. In this work, we test three\nhypotheses concerning the effects of action taking as an auxiliary task in\nmodelling iCR policies. Contrary to initial expectations, we conclude that its\ncontribution to learning an iCR policy is limited, but some information can\nstill be extracted from prediction uncertainty. We present further evidence\nthat even well-motivated, Transformer-based models fail to learn good policies\nfor when to ask Instruction CRs (iCRs), while the task of determining what to\nask about can be more successfully modelled. Considering the implications of\nthese findings, we further discuss the shortcomings of the data-driven paradigm\nfor learning meta-communication acts.\n",
        "authors": "Brielen Madureira, David Schlangen",
        "authors_parsed": [
            [
                "Madureira",
                "Brielen",
                ""
            ],
            [
                "Schlangen",
                "David",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "Accepted to UnImplicit workshop at EACL 2024",
        "doi": null,
        "id": "2401.17039",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Brielen Madureira",
        "title": "Taking Action Towards Graceful Interaction: The Effects of Performing\n  Actions on Modelling Policies for Instruction Clarification Requests",
        "update_date": "2024-01-31",
        "versions": [
            {
                "created": "Tue, 30 Jan 2024 14:18:31 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Transformer-based pre-trained models with millions of parameters require\nlarge storage. Recent approaches tackle this shortcoming by training adapters,\nbut these approaches still require a relatively large number of parameters. In\nthis study, AdapterBias, a surprisingly simple yet effective adapter\narchitecture, is proposed. AdapterBias adds a token-dependent shift to the\nhidden output of transformer layers to adapt to downstream tasks with only a\nvector and a linear layer. Extensive experiments are conducted to demonstrate\nthe effectiveness of AdapterBias. The experiments show that our proposed method\ncan dramatically reduce the trainable parameters compared to the previous works\nwith a minimal decrease in task performances compared with fine-tuned\npre-trained models. We further find that AdapterBias automatically learns to\nassign more significant representation shifts to the tokens related to the task\nin consideration.\n",
        "authors": "Chin-Lun Fu, Zih-Ching Chen, Yun-Ru Lee, Hung-yi Lee",
        "authors_parsed": [
            [
                "Fu",
                "Chin-Lun",
                ""
            ],
            [
                "Chen",
                "Zih-Ching",
                ""
            ],
            [
                "Lee",
                "Yun-Ru",
                ""
            ],
            [
                "Lee",
                "Hung-yi",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "Findings of NAACL 2022",
        "doi": null,
        "id": "2205.00305",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Chin-Lun Fu",
        "title": "AdapterBias: Parameter-efficient Token-dependent Representation Shift\n  for Adapters in NLP Tasks",
        "update_date": "2023-01-31",
        "versions": [
            {
                "created": "Sat, 30 Apr 2022 16:49:41 GMT",
                "version": "v1"
            },
            {
                "created": "Thu, 4 Aug 2022 01:47:37 GMT",
                "version": "v2"
            },
            {
                "created": "Mon, 10 Oct 2022 10:16:36 GMT",
                "version": "v3"
            },
            {
                "created": "Mon, 30 Jan 2023 12:38:51 GMT",
                "version": "v4"
            }
        ]
    },
    {
        "abstract": "  Public opinion reflects and shapes societal behavior, but the traditional\nsurvey-based tools to measure it are limited. We introduce a novel approach to\nprobe media diet models -- language models adapted to online news, TV\nbroadcast, or radio show content -- that can emulate the opinions of\nsubpopulations that have consumed a set of media. To validate this method, we\nuse as ground truth the opinions expressed in U.S. nationally representative\nsurveys on COVID-19 and consumer confidence. Our studies indicate that this\napproach is (1) predictive of human judgements found in survey response\ndistributions and robust to phrasing and channels of media exposure, (2) more\naccurate at modeling people who follow media more closely, and (3) aligned with\nliterature on which types of opinions are affected by media consumption.\nProbing language models provides a powerful new method for investigating media\neffects, has practical applications in supplementing polls and forecasting\npublic opinion, and suggests a need for further study of the surprising\nfidelity with which neural language models can predict human responses.\n",
        "authors": "Eric Chu, Jacob Andreas, Stephen Ansolabehere, Deb Roy",
        "authors_parsed": [
            [
                "Chu",
                "Eric",
                ""
            ],
            [
                "Andreas",
                "Jacob",
                ""
            ],
            [
                "Ansolabehere",
                "Stephen",
                ""
            ],
            [
                "Roy",
                "Deb",
                ""
            ]
        ],
        "categories": "cs.CL cs.LG",
        "comments": null,
        "doi": null,
        "id": "2303.16779",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Eric Chu",
        "title": "Language Models Trained on Media Diets Can Predict Public Opinion",
        "update_date": "2023-03-30",
        "versions": [
            {
                "created": "Tue, 28 Mar 2023 06:08:25 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Hallucination in a foundation model (FM) refers to the generation of content\nthat strays from factual reality or includes fabricated information. This\nsurvey paper provides an extensive overview of recent efforts that aim to\nidentify, elucidate, and tackle the problem of hallucination, with a particular\nfocus on ``Large'' Foundation Models (LFMs). The paper classifies various types\nof hallucination phenomena that are specific to LFMs and establishes evaluation\ncriteria for assessing the extent of hallucination. It also examines existing\nstrategies for mitigating hallucination in LFMs and discusses potential\ndirections for future research in this area. Essentially, the paper offers a\ncomprehensive examination of the challenges and solutions related to\nhallucination in LFMs.\n",
        "authors": "Vipula Rawte, Amit Sheth, Amitava Das",
        "authors_parsed": [
            [
                "Rawte",
                "Vipula",
                ""
            ],
            [
                "Sheth",
                "Amit",
                ""
            ],
            [
                "Das",
                "Amitava",
                ""
            ]
        ],
        "categories": "cs.AI cs.CL cs.IR",
        "comments": null,
        "doi": null,
        "id": "2309.05922",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Vipula Rawte",
        "title": "A Survey of Hallucination in Large Foundation Models",
        "update_date": "2023-09-13",
        "versions": [
            {
                "created": "Tue, 12 Sep 2023 02:34:06 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  The proliferation of automated conversational systems such as chatbots,\nspoken-dialogue systems, and smart speakers, has significantly impacted modern\ndigital life. However, these systems are primarily designed to provide answers\nto well-defined questions rather than to support users in exploring complex,\nill-defined questions. In this paper, we aim to push the boundaries of\nconversational systems by examining the types of nebulous, open-ended questions\nthat can best be answered through conversation. We first sampled 500 questions\nfrom one million open-ended requests posted on AskReddit, and then recruited\nonline crowd workers to answer eight inquiries about these questions. We also\nperformed open coding to categorize the questions into 27 different domains. We\nfound that the issues people believe require conversation to resolve\nsatisfactorily are highly social and personal. Our work provides insights into\nhow future research could be geared to align with users' needs.\n",
        "authors": "Shih-Hong Huang, Chieh-Yang Huang, Ya-Fang Lin, Ting-Hao 'Kenneth'\n  Huang",
        "authors_parsed": [
            [
                "Huang",
                "Shih-Hong",
                ""
            ],
            [
                "Huang",
                "Chieh-Yang",
                ""
            ],
            [
                "Lin",
                "Ya-Fang",
                ""
            ],
            [
                "Huang",
                "Ting-Hao 'Kenneth'",
                ""
            ]
        ],
        "categories": "cs.HC cs.CL",
        "comments": "To appear in CHI 2023 Late-Breaking Work",
        "doi": null,
        "id": "2303.17710",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Shih-Hong Huang",
        "title": "What Types of Questions Require Conversation to Answer? A Case Study of\n  AskReddit Questions",
        "update_date": "2023-04-05",
        "versions": [
            {
                "created": "Thu, 30 Mar 2023 21:05:22 GMT",
                "version": "v1"
            },
            {
                "created": "Mon, 3 Apr 2023 21:14:54 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  There has recently been growing interest in the automatic generation of\ncooking recipes that satisfy some form of dietary restrictions, thanks in part\nto the availability of online recipe data. Prior studies have used pre-trained\nlanguage models, or relied on small paired recipe data (e.g., a recipe paired\nwith a similar one that satisfies a dietary constraint). However, pre-trained\nlanguage models generate inconsistent or incoherent recipes, and paired\ndatasets are not available at scale. We address these deficiencies with\nRecipeCrit, a hierarchical denoising auto-encoder that edits recipes given\ningredient-level critiques. The model is trained for recipe completion to learn\nsemantic relationships within recipes. Our work's main innovation is our\nunsupervised critiquing module that allows users to edit recipes by interacting\nwith the predicted ingredients; the system iteratively rewrites recipes to\nsatisfy users' feedback. Experiments on the Recipe1M recipe dataset show that\nour model can more effectively edit recipes compared to strong\nlanguage-modeling baselines, creating recipes that satisfy user constraints and\nare more correct, serendipitous, coherent, and relevant as measured by human\njudges.\n",
        "authors": "Diego Antognini, Shuyang Li, Boi Faltings, Julian McAuley",
        "authors_parsed": [
            [
                "Antognini",
                "Diego",
                ""
            ],
            [
                "Li",
                "Shuyang",
                ""
            ],
            [
                "Faltings",
                "Boi",
                ""
            ],
            [
                "McAuley",
                "Julian",
                ""
            ]
        ],
        "categories": "cs.CL cs.IR cs.LG",
        "comments": "Accepted at EACL 2023. 10 pages, 2 figures, 6 tables, 1 algorithm",
        "doi": null,
        "id": "2205.02454",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Diego Antognini",
        "title": "Assistive Recipe Editing through Critiquing",
        "update_date": "2023-01-27",
        "versions": [
            {
                "created": "Thu, 5 May 2022 05:52:27 GMT",
                "version": "v1"
            },
            {
                "created": "Thu, 26 Jan 2023 15:41:56 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  As large language models (LLM) evolve in their capabilities, various recent\nstudies have tried to quantify their behavior using psychological tools created\nto study human behavior. One such example is the measurement of \"personality\"\nof LLMs using self-assessment personality tests developed to measure human\npersonality. Yet almost none of these works verify the applicability of these\ntests on LLMs. In this paper, we analyze the reliability of LLM personality\nscores obtained from self-assessment personality tests using two simple\nexperiments. We first introduce the property of prompt sensitivity, where three\nsemantically equivalent prompts representing three intuitive ways of\nadministering self-assessment tests on LLMs are used to measure the personality\nof the same LLM. We find that all three prompts lead to very different\npersonality scores, a difference that is statistically significant for all\ntraits in a large majority of scenarios. We then introduce the property of\noption-order symmetry for personality measurement of LLMs. Since most of the\nself-assessment tests exist in the form of multiple choice question (MCQ)\nquestions, we argue that the scores should also be robust to not just the\nprompt template but also the order in which the options are presented. This\ntest unsurprisingly reveals that the self-assessment test scores are not robust\nto the order of the options. These simple tests, done on ChatGPT and three\nLlama2 models of different sizes, show that self-assessment personality tests\ncreated for humans are unreliable measures of personality in LLMs.\n",
        "authors": "Akshat Gupta, Xiaoyang Song, Gopala Anumanchipalli",
        "authors_parsed": [
            [
                "Gupta",
                "Akshat",
                ""
            ],
            [
                "Song",
                "Xiaoyang",
                ""
            ],
            [
                "Anumanchipalli",
                "Gopala",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI",
        "comments": null,
        "doi": null,
        "id": "2309.08163",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Akshat Gupta",
        "title": "Self-Assessment Tests are Unreliable Measures of LLM Personality",
        "update_date": "2024-01-04",
        "versions": [
            {
                "created": "Fri, 15 Sep 2023 05:19:39 GMT",
                "version": "v1"
            },
            {
                "created": "Tue, 2 Jan 2024 23:00:41 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Knowledge-based dialogue systems with internet retrieval have recently\nattracted considerable attention from researchers. The dialogue systems\novercome a major limitation of traditional knowledge dialogue systems, where\nthe timeliness of knowledge cannot be assured, hence providing greater\npractical application value. Knowledge-based dialogue systems with internet\nretrieval can be typically segmented into three tasks: Retrieval Decision,\nQuery Generation, and Response Generation. However, many of studies assumed\nthat all conversations require external knowledge to continue, neglecting the\ncritical step of determining when retrieval is necessary. This assumption often\nleads to an over-dependence on external knowledge, even when it may not be\nrequired. Our work addresses this oversight by employing a single unified model\nfacilitated by prompt and multi-task learning approaches. This model not only\ndecides whether retrieval is necessary but also generates retrieval queries and\nresponses. By integrating these functions, our system leverages the full\npotential of pre-trained models and reduces the complexity and costs associated\nwith deploying multiple models. We conducted extensive experiments to\ninvestigate the mutual enhancement among the three tasks in our system. What is\nmore, the experiment results on the Wizint and Dusinc datasets not only\ndemonstrate that our unified model surpasses the baseline performance for\nindividual tasks, but also reveal that it achieves comparable results when\ncontrasted with SOTA systems that deploy separate, specialized models for each\ntask.\n",
        "authors": "Zhongtian Hu, Yangqi Chen, Meng Zhao, Ronghan Li, Lifang Wang",
        "authors_parsed": [
            [
                "Hu",
                "Zhongtian",
                ""
            ],
            [
                "Chen",
                "Yangqi",
                ""
            ],
            [
                "Zhao",
                "Meng",
                ""
            ],
            [
                "Li",
                "Ronghan",
                ""
            ],
            [
                "Wang",
                "Lifang",
                ""
            ]
        ],
        "categories": "cs.IR cs.AI cs.CL",
        "comments": null,
        "doi": null,
        "id": "2401.06811",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Zhongtian Hu",
        "title": "UniRQR: A Unified Model for Retrieval Decision, Query, and Response\n  Generation in Internet-Based Knowledge Dialogue Systems",
        "update_date": "2024-01-17",
        "versions": [
            {
                "created": "Thu, 11 Jan 2024 06:09:15 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Text-to-SQL systems (also known as NL-to-SQL systems) have become an\nincreasingly popular solution for bridging the gap between user capabilities\nand SQL-based data access. These systems translate user requests in natural\nlanguage to valid SQL statements for a specific database. Recent Text-to-SQL\nsystems have benefited from the rapid improvement of transformer-based language\nmodels. However, while Text-to-SQL systems that incorporate such models\ncontinuously reach new high scores on -- often synthetic -- benchmark datasets,\na systematic exploration of their robustness towards different data models in a\nreal-world, realistic scenario is notably missing. This paper provides the\nfirst in-depth evaluation of the data model robustness of Text-to-SQL systems\nin practice based on a multi-year international project focused on Text-to-SQL\ninterfaces. Our evaluation is based on a real-world deployment of FootballDB, a\nsystem that was deployed over a 9 month period in the context of the FIFA World\nCup 2022, during which about 6K natural language questions were asked and\nexecuted. All of our data is based on real user questions that were asked live\nto the system. We manually labeled and translated a subset of these questions\nfor three different data models. For each data model, we explore the\nperformance of representative Text-to-SQL systems and language models. We\nfurther quantify the impact of training data size, pre-, and post-processing\nsteps as well as language model inference time. Our comprehensive evaluation\nsheds light on the design choices of real-world Text-to-SQL systems and their\nimpact on moving from research prototypes to real deployments. Last, we provide\na new benchmark dataset to the community, which is the first to enable the\nevaluation of different data models for the same dataset and is substantially\nmore challenging than most previous datasets in terms of query complexity.\n",
        "authors": "Jonathan F\\\"urst, Catherine Kosten, Farhard Nooralahzadeh, Yi Zhang,\n  Kurt Stockinger",
        "authors_parsed": [
            [
                "F\u00fcrst",
                "Jonathan",
                ""
            ],
            [
                "Kosten",
                "Catherine",
                ""
            ],
            [
                "Nooralahzadeh",
                "Farhard",
                ""
            ],
            [
                "Zhang",
                "Yi",
                ""
            ],
            [
                "Stockinger",
                "Kurt",
                ""
            ]
        ],
        "categories": "cs.DB cs.AI cs.CL",
        "comments": null,
        "doi": null,
        "id": "2402.08349",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Jonathan F\\\"urst",
        "title": "Evaluating the Data Model Robustness of Text-to-SQL Systems Based on\n  Real User Queries",
        "update_date": "2024-02-14",
        "versions": [
            {
                "created": "Tue, 13 Feb 2024 10:28:57 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Large Language Models (LLMs) have been demonstrating the ability to solve\ncomplex tasks by delivering answers that are positively evaluated by humans due\nin part to the intensive use of human feedback that refines responses. However,\nthe suggestibility transmitted through human feedback increases the inclination\nto produce responses that correspond to the user's beliefs or misleading\nprompts as opposed to true facts, a behaviour known as sycophancy. This\nphenomenon decreases the bias, robustness, and, consequently, their\nreliability.\n  In this paper, we shed light on the suggestibility of LLMs to sycophantic\nbehaviour, demonstrating these tendencies via human-influenced prompts over\ndifferent tasks. Our investigation reveals that LLMs show sycophantic\ntendencies when responding to queries involving subjective opinions and\nstatements that should elicit a contrary response based on facts, demonstrating\na lack of robustness.\n",
        "authors": "Leonardo Ranaldi and Giulia Pucci",
        "authors_parsed": [
            [
                "Ranaldi",
                "Leonardo",
                ""
            ],
            [
                "Pucci",
                "Giulia",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI",
        "comments": null,
        "doi": null,
        "id": "2311.09410",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Leonardo Ranaldi Mr",
        "title": "When Large Language Models contradict humans? Large Language Models'\n  Sycophantic Behaviour",
        "update_date": "2023-11-17",
        "versions": [
            {
                "created": "Wed, 15 Nov 2023 22:18:33 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Factual recall from a reference source is crucial for evaluating the\nperformance of Retrieval Augmented Generation (RAG) systems, as it directly\nprobes into the quality of both retrieval and generation. However, it still\nremains a challenge to perform this evaluation reliably and efficiently. Recent\nwork has focused on fact verification via prompting language model (LM)\nevaluators, however we demonstrate that these methods are unreliable in the\npresence of incomplete or inaccurate information. We introduce Facts as a\nFunction (FaaF), a new approach to fact verification that utilizes the function\ncalling abilities of LMs and a framework for RAG factual recall evaluation.\nFaaF substantially improves the ability of LMs to identify unsupported facts in\ntext with incomplete information whilst improving efficiency and lowering cost\nby several times, compared to prompt-based approaches.\n",
        "authors": "Vasileios Katranidis and Gabor Barany",
        "authors_parsed": [
            [
                "Katranidis",
                "Vasileios",
                ""
            ],
            [
                "Barany",
                "Gabor",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "12 pages, 3 figures",
        "doi": null,
        "id": "2403.03888",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Vasileios Katranidis",
        "title": "FaaF: Facts as a Function for the evaluation of RAG systems",
        "update_date": "2024-03-07",
        "versions": [
            {
                "created": "Wed, 6 Mar 2024 17:48:06 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  State-of-the-art large-scale universal speech models (USMs) show a decent\nautomatic speech recognition (ASR) performance across multiple domains and\nlanguages. However, it remains a challenge for these models to recognize\noverlapped speech, which is often seen in meeting conversations. We propose an\napproach to adapt USMs for multi-talker ASR. We first develop an enhanced\nversion of serialized output training to jointly perform multi-talker ASR and\nutterance timestamp prediction. That is, we predict the ASR hypotheses for all\nspeakers, count the speakers, and estimate the utterance timestamps at the same\ntime. We further introduce a lightweight adapter module to maintain the\nmultilingual property of the USMs even when we perform the adaptation with only\na single language. Experimental results obtained using the AMI and AliMeeting\ncorpora show that our proposed approach effectively transfers the USMs to a\nstrong multilingual multi-talker ASR model with timestamp prediction\ncapability.\n",
        "authors": "Chenda Li, Yao Qian, Zhuo Chen, Naoyuki Kanda, Dongmei Wang, Takuya\n  Yoshioka, Yanmin Qian, and Michael Zeng",
        "authors_parsed": [
            [
                "Li",
                "Chenda",
                ""
            ],
            [
                "Qian",
                "Yao",
                ""
            ],
            [
                "Chen",
                "Zhuo",
                ""
            ],
            [
                "Kanda",
                "Naoyuki",
                ""
            ],
            [
                "Wang",
                "Dongmei",
                ""
            ],
            [
                "Yoshioka",
                "Takuya",
                ""
            ],
            [
                "Qian",
                "Yanmin",
                ""
            ],
            [
                "Zeng",
                "Michael",
                ""
            ]
        ],
        "categories": "eess.AS cs.CL",
        "comments": "Accepted by Interspeech 2023",
        "doi": null,
        "id": "2305.18747",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Chenda Li",
        "title": "Adapting Multi-Lingual ASR Models for Handling Multiple Talkers",
        "update_date": "2023-05-31",
        "versions": [
            {
                "created": "Tue, 30 May 2023 05:05:52 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Out-of-distribution (OOD) detection is essential for the reliable and safe\ndeployment of machine learning systems in the real world. Great progress has\nbeen made over the past years. This paper presents the first review of recent\nadvances in OOD detection with a particular focus on natural language\nprocessing approaches. First, we provide a formal definition of OOD detection\nand discuss several related fields. We then categorize recent algorithms into\nthree classes according to the data they used: (1) OOD data available, (2) OOD\ndata unavailable + in-distribution (ID) label available, and (3) OOD data\nunavailable + ID label unavailable. Third, we introduce datasets, applications,\nand metrics. Finally, we summarize existing work and present potential future\nresearch topics.\n",
        "authors": "Hao Lang, Yinhe Zheng, Yixuan Li, Jian Sun, Fei Huang, Yongbin Li",
        "authors_parsed": [
            [
                "Lang",
                "Hao",
                ""
            ],
            [
                "Zheng",
                "Yinhe",
                ""
            ],
            [
                "Li",
                "Yixuan",
                ""
            ],
            [
                "Sun",
                "Jian",
                ""
            ],
            [
                "Huang",
                "Fei",
                ""
            ],
            [
                "Li",
                "Yongbin",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI cs.LG",
        "comments": "TMLR",
        "doi": null,
        "id": "2305.03236",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Hao Lang",
        "title": "A Survey on Out-of-Distribution Detection in NLP",
        "update_date": "2023-12-29",
        "versions": [
            {
                "created": "Fri, 5 May 2023 01:38:49 GMT",
                "version": "v1"
            },
            {
                "created": "Wed, 27 Dec 2023 07:15:20 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Despite the impressive capabilities of large language models (LLMs) across\ndiverse applications, they still suffer from trustworthiness issues, such as\nhallucinations and misalignments. Retrieval-augmented language models (RAG)\nhave been proposed to enhance the credibility of generations by grounding\nexternal knowledge, but the theoretical understandings of their generation\nrisks remains unexplored. In this paper, we answer: 1) whether RAG can indeed\nlead to low generation risks, 2) how to provide provable guarantees on the\ngeneration risks of RAG and vanilla LLMs, and 3) what sufficient conditions\nenable RAG models to reduce generation risks. We propose C-RAG, the first\nframework to certify generation risks for RAG models. Specifically, we provide\nconformal risk analysis for RAG models and certify an upper confidence bound of\ngeneration risks, which we refer to as conformal generation risk. We also\nprovide theoretical guarantees on conformal generation risks for general\nbounded risk functions under test distribution shifts. We prove that RAG\nachieves a lower conformal generation risk than that of a single LLM when the\nquality of the retrieval model and transformer is non-trivial. Our intensive\nempirical results demonstrate the soundness and tightness of our conformal\ngeneration risk guarantees across four widely-used NLP datasets on four\nstate-of-the-art retrieval models.\n",
        "authors": "Mintong Kang, Nezihe Merve G\\\"urel, Ning Yu, Dawn Song, Bo Li",
        "authors_parsed": [
            [
                "Kang",
                "Mintong",
                ""
            ],
            [
                "G\u00fcrel",
                "Nezihe Merve",
                ""
            ],
            [
                "Yu",
                "Ning",
                ""
            ],
            [
                "Song",
                "Dawn",
                ""
            ],
            [
                "Li",
                "Bo",
                ""
            ]
        ],
        "categories": "cs.AI cs.CL cs.IR",
        "comments": null,
        "doi": null,
        "id": "2402.03181",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Mintong Kang",
        "title": "C-RAG: Certified Generation Risks for Retrieval-Augmented Language\n  Models",
        "update_date": "2024-03-05",
        "versions": [
            {
                "created": "Mon, 5 Feb 2024 16:46:16 GMT",
                "version": "v1"
            },
            {
                "created": "Mon, 12 Feb 2024 22:19:17 GMT",
                "version": "v2"
            },
            {
                "created": "Sun, 3 Mar 2024 18:13:54 GMT",
                "version": "v3"
            }
        ]
    },
    {
        "abstract": "  The emergence of pretrained large language models has led to the deployment\nof a range of social chatbots for chitchat. Although these chatbots demonstrate\nlanguage ability and fluency, they are not guaranteed to be engaging and can\nstruggle to retain users. This work investigates the development of social\nchatbots that prioritize user engagement to enhance retention, specifically\nexamining the use of human feedback to efficiently develop highly engaging\nchatbots. The proposed approach uses automatic pseudo-labels collected from\nuser interactions to train a reward model that can be used to reject\nlow-scoring sample responses generated by the chatbot model at inference time.\nIntuitive evaluation metrics, such as mean conversation length (MCL), are\nintroduced as proxies to measure the level of engagement of deployed chatbots.\nA/B testing on groups of 10,000 new daily chatbot users on the Chai Research\nplatform shows that this approach increases the MCL by up to 70%, which\ntranslates to a more than 30% increase in user retention for a GPT-J 6B model.\nFuture work aims to use the reward model to realise a data fly-wheel, where the\nlatest user conversations can be used to alternately fine-tune the language\nmodel and the reward model.\n",
        "authors": "Robert Irvine, Douglas Boubert, Vyas Raina, Adian Liusie, Ziyi Zhu,\n  Vineet Mudupalli, Aliaksei Korshuk, Zongyi Liu, Fritz Cremer, Valentin\n  Assassi, Christie-Carol Beauchamp, Xiaoding Lu, Thomas Rialan, William\n  Beauchamp",
        "authors_parsed": [
            [
                "Irvine",
                "Robert",
                ""
            ],
            [
                "Boubert",
                "Douglas",
                ""
            ],
            [
                "Raina",
                "Vyas",
                ""
            ],
            [
                "Liusie",
                "Adian",
                ""
            ],
            [
                "Zhu",
                "Ziyi",
                ""
            ],
            [
                "Mudupalli",
                "Vineet",
                ""
            ],
            [
                "Korshuk",
                "Aliaksei",
                ""
            ],
            [
                "Liu",
                "Zongyi",
                ""
            ],
            [
                "Cremer",
                "Fritz",
                ""
            ],
            [
                "Assassi",
                "Valentin",
                ""
            ],
            [
                "Beauchamp",
                "Christie-Carol",
                ""
            ],
            [
                "Lu",
                "Xiaoding",
                ""
            ],
            [
                "Rialan",
                "Thomas",
                ""
            ],
            [
                "Beauchamp",
                "William",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI cs.LG",
        "comments": null,
        "doi": null,
        "id": "2303.06135",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Douglas Boubert",
        "title": "Rewarding Chatbots for Real-World Engagement with Millions of Users",
        "update_date": "2023-04-03",
        "versions": [
            {
                "created": "Fri, 10 Mar 2023 18:53:52 GMT",
                "version": "v1"
            },
            {
                "created": "Thu, 30 Mar 2023 18:28:05 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Sentence embeddings induced with various transformer architectures encode\nmuch semantic and syntactic information in a distributed manner in a\none-dimensional array. We investigate whether specific grammatical information\ncan be accessed in these distributed representations. Using data from a task\ndeveloped to test rule-like generalizations, our experiments on detecting\nsubject-verb agreement yield several promising results. First, we show that\nwhile the usual sentence representations encoded as one-dimensional arrays do\nnot easily support extraction of rule-like regularities, a two-dimensional\nreshaping of these vectors allows various learning architectures to access such\ninformation. Next, we show that various architectures can detect patterns in\nthese two-dimensional reshaped sentence embeddings and successfully learn a\nmodel based on smaller amounts of simpler training data, which performs well on\nmore complex test data. This indicates that current sentence embeddings contain\ninformation that is regularly distributed, and which can be captured when the\nembeddings are reshaped into higher dimensional arrays. Our results cast light\non representations produced by language models and help move towards developing\nfew-shot learning approaches.\n",
        "authors": "Vivi Nastase and Paola Merlo",
        "authors_parsed": [
            [
                "Nastase",
                "Vivi",
                ""
            ],
            [
                "Merlo",
                "Paola",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "Published in RepL4NLP 2023",
        "doi": null,
        "id": "2312.09890",
        "journal-ref": "Proceedings of the 8th Workshop on Representation Learning for NLP\n  (RepL4NLP 2023)",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "report-no": null,
        "submitter": "Vivi Nastase",
        "title": "Grammatical information in BERT sentence embeddings as two-dimensional\n  arrays",
        "update_date": "2023-12-18",
        "versions": [
            {
                "created": "Fri, 15 Dec 2023 15:41:52 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  A test oracle serves as a criterion or mechanism to assess the correspondence\nbetween software output and the anticipated behavior for a given input set. In\nautomated testing, black-box techniques, known for their non-intrusive nature\nin test oracle construction, are widely used, including notable methodologies\nlike differential testing and metamorphic testing. Inspired by the mathematical\nconcept of inverse function, we present Retromorphic Testing, a novel black-box\ntesting methodology. It leverages an auxiliary program in conjunction with the\nprogram under test, which establishes a dual-program structure consisting of a\nforward program and a backward program. The input data is first processed by\nthe forward program and then its program output is reversed to its original\ninput format using the backward program. In particular, the auxiliary program\ncan operate as either the forward or backward program, leading to different\ntesting modes. The process concludes by examining the relationship between the\ninitial input and the transformed output within the input domain. For example,\nto test the implementation of the sine function $\\sin(x)$, we can employ its\ninverse function, $\\arcsin(x)$, and validate the equation $x =\n\\sin(\\arcsin(x)+2k\\pi), \\forall k \\in \\mathbb{Z}$. In addition to the\nhigh-level concept of Retromorphic Testing, this paper presents its three\ntesting modes with illustrative use cases across diverse programs, including\nalgorithms, traditional software, and AI applications.\n",
        "authors": "Boxi Yu, Qiuyang Mang, Qingshuo Guo, Pinjia He",
        "authors_parsed": [
            [
                "Yu",
                "Boxi",
                ""
            ],
            [
                "Mang",
                "Qiuyang",
                ""
            ],
            [
                "Guo",
                "Qingshuo",
                ""
            ],
            [
                "He",
                "Pinjia",
                ""
            ]
        ],
        "categories": "cs.SE cs.AI cs.CL cs.CV",
        "comments": null,
        "doi": null,
        "id": "2310.06433",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Boxi Yu",
        "title": "Retromorphic Testing: A New Approach to the Test Oracle Problem",
        "update_date": "2023-10-11",
        "versions": [
            {
                "created": "Tue, 10 Oct 2023 09:03:01 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  A common way to explore text corpora is through low-dimensional projections\nof the documents, where one hopes that thematically similar documents will be\nclustered together in the projected space. However, popular algorithms for\ndimensionality reduction of text corpora, like Latent Dirichlet Allocation\n(LDA), often produce projections that do not capture human notions of document\nsimilarity. We propose a semi-supervised human-in-the-loop LDA-based method for\nlearning topics that preserve semantically meaningful relationships between\ndocuments in low-dimensional projections. On synthetic corpora, our method\nyields more interpretable projections than baseline methods with only a\nfraction of labels provided. On a real corpus, we obtain qualitatively similar\nresults.\n",
        "authors": "Charumathi Badrinath, Weiwei Pan, Finale Doshi-Velez",
        "authors_parsed": [
            [
                "Badrinath",
                "Charumathi",
                ""
            ],
            [
                "Pan",
                "Weiwei",
                ""
            ],
            [
                "Doshi-Velez",
                "Finale",
                ""
            ]
        ],
        "categories": "cs.CL cs.LG",
        "comments": null,
        "doi": null,
        "id": "2308.01420",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Charumathi Badrinath",
        "title": "SAP-sLDA: An Interpretable Interface for Exploring Unstructured Text",
        "update_date": "2023-08-04",
        "versions": [
            {
                "created": "Fri, 28 Jul 2023 05:43:39 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  We present our work on predicting United Nations sustainable development\ngoals (SDG) for university courses. We use an LLM named PaLM 2 to generate\ntraining data given a noisy human-authored course description input as input.\nWe use this data to train several different smaller language models to predict\nSDGs for university courses. This work contributes to better university level\nadaptation of SDGs. The best performing model in our experiments was BART with\nan F1-score of 0.786.\n",
        "authors": "Lev Kharlashkin, Melany Macias, Leo Huovinen, Mika H\\\"am\\\"al\\\"ainen",
        "authors_parsed": [
            [
                "Kharlashkin",
                "Lev",
                ""
            ],
            [
                "Macias",
                "Melany",
                ""
            ],
            [
                "Huovinen",
                "Leo",
                ""
            ],
            [
                "H\u00e4m\u00e4l\u00e4inen",
                "Mika",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "3 figures, 2 tables",
        "doi": null,
        "id": "2402.16420",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Lev Kharlashkin",
        "title": "Predicting Sustainable Development Goals Using Course Descriptions --\n  from LLMs to Conventional Foundation Models",
        "update_date": "2024-02-27",
        "versions": [
            {
                "created": "Mon, 26 Feb 2024 09:19:46 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  We present Visual Knowledge oriented Programming platform (VisKoP), a\nknowledge base question answering (KBQA) system that integrates human into the\nloop to edit and debug the knowledge base (KB) queries. VisKoP not only\nprovides a neural program induction module, which converts natural language\nquestions into knowledge oriented program language (KoPL), but also maps KoPL\nprograms into graphical elements. KoPL programs can be edited with simple\ngraphical operators, such as dragging to add knowledge operators and slot\nfilling to designate operator arguments. Moreover, VisKoP provides\nauto-completion for its knowledge base schema and users can easily debug the\nKoPL program by checking its intermediate results. To facilitate the practical\nKBQA on a million-entity-level KB, we design a highly efficient KoPL execution\nengine for the back-end. Experiment results show that VisKoP is highly\nefficient and user interaction can fix a large portion of wrong KoPL programs\nto acquire the correct answer. The VisKoP online demo\nhttps://demoviskop.xlore.cn (Stable release of this paper) and\nhttps://viskop.xlore.cn (Beta release with new features), highly efficient KoPL\nengine https://pypi.org/project/kopl-engine, and screencast video\nhttps://youtu.be/zAbJtxFPTXo are now publicly available.\n",
        "authors": "Zijun Yao, Yuanyong Chen, Xin Lv, Shulin Cao, Amy Xin, Jifan Yu,\n  Hailong Jin, Jianjun Xu, Peng Zhang, Lei Hou, Juanzi Li",
        "authors_parsed": [
            [
                "Yao",
                "Zijun",
                ""
            ],
            [
                "Chen",
                "Yuanyong",
                ""
            ],
            [
                "Lv",
                "Xin",
                ""
            ],
            [
                "Cao",
                "Shulin",
                ""
            ],
            [
                "Xin",
                "Amy",
                ""
            ],
            [
                "Yu",
                "Jifan",
                ""
            ],
            [
                "Jin",
                "Hailong",
                ""
            ],
            [
                "Xu",
                "Jianjun",
                ""
            ],
            [
                "Zhang",
                "Peng",
                ""
            ],
            [
                "Hou",
                "Lei",
                ""
            ],
            [
                "Li",
                "Juanzi",
                ""
            ]
        ],
        "categories": "cs.CL cs.HC",
        "comments": null,
        "doi": null,
        "id": "2307.03130",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Zijun Yao",
        "title": "VisKoP: Visual Knowledge oriented Programming for Interactive Knowledge\n  Base Question Answering",
        "update_date": "2023-07-07",
        "versions": [
            {
                "created": "Thu, 6 Jul 2023 16:58:27 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Large language models (LLMs) have exhibited impressive performance in\nlanguage comprehension and various reasoning tasks. However, their abilities in\nspatial reasoning, a crucial aspect of human cognition, remain relatively\nunexplored. Human possess a remarkable ability to create mental images of\nunseen objects and actions through a process known as \\textbf{the Mind's Eye},\nenabling the imagination of the unseen world. Inspired by this cognitive\ncapacity, we propose Visualization-of-Thought (\\textbf{VoT}) prompting. VoT\naims to elicit spatial reasoning of LLMs by visualizing their reasoning traces,\nthereby guiding subsequent reasoning steps. We employed VoT for multi-hop\nspatial reasoning tasks, including natural language navigation, visual\nnavigation, and visual tiling in 2D grid worlds. Experimental results\ndemonstrated that VoT significantly enhances the spatial reasoning abilities of\nLLMs. Notably, VoT outperformed existing multimodal large language models\n(MLLMs) in these tasks. While VoT works surprisingly well on LLMs, the ability\nto generate \\textit{mental images} to facilitate spatial reasoning resembles\nthe mind's eye process, suggesting its potential viability in MLLMs.\n",
        "authors": "Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui,\n  Furu Wei",
        "authors_parsed": [
            [
                "Wu",
                "Wenshan",
                ""
            ],
            [
                "Mao",
                "Shaoguang",
                ""
            ],
            [
                "Zhang",
                "Yadong",
                ""
            ],
            [
                "Xia",
                "Yan",
                ""
            ],
            [
                "Dong",
                "Li",
                ""
            ],
            [
                "Cui",
                "Lei",
                ""
            ],
            [
                "Wei",
                "Furu",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": null,
        "doi": null,
        "id": "2404.03622",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Wenshan Wu",
        "title": "Visualization-of-Thought Elicits Spatial Reasoning in Large Language\n  Models",
        "update_date": "2024-04-05",
        "versions": [
            {
                "created": "Thu, 4 Apr 2024 17:45:08 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  This position paper discusses the problem of multilingual evaluation. Using\nsimple statistics, such as average language performance, might inject\nlinguistic biases in favor of dominant language families into evaluation\nmethodology. We argue that a qualitative analysis informed by comparative\nlinguistics is needed for multilingual results to detect this kind of bias. We\nshow in our case study that results in published works can indeed be\nlinguistically biased and we demonstrate that visualization based on URIEL\ntypological database can detect it.\n",
        "authors": "Mat\\'u\\v{s} Pikuliak and Mari\\'an \\v{S}imko",
        "authors_parsed": [
            [
                "Pikuliak",
                "Mat\u00fa\u0161",
                ""
            ],
            [
                "\u0160imko",
                "Mari\u00e1n",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "The 2022 Workshop on Multilingual Representation Learning",
        "doi": null,
        "id": "2301.01269",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Mat\\'u\\v{s} Pikuliak",
        "title": "Average Is Not Enough: Caveats of Multilingual Evaluation",
        "update_date": "2023-01-04",
        "versions": [
            {
                "created": "Tue, 3 Jan 2023 18:23:42 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up\nthe learning capacity of neural networks, however, they have issues like (a)\nHigh Memory Usage, due to duplication of the network layers into multiple\ncopies as experts; and (b) Redundancy in Experts, as common learning-based\nrouting policies suffer from representational collapse. Therefore, vanilla SMoE\nmodels are memory inefficient and non-scalable, especially for\nresource-constrained downstream scenarios. In this paper, we ask: Can we craft\na compact SMoE model by consolidating expert information? What is the best\nrecipe to merge multiple experts into fewer but more knowledgeable experts? Our\npilot investigation reveals that conventional model merging methods fail to be\neffective in such expert merging for SMoE. The potential reasons are: (1)\nredundant information overshadows critical experts; (2) appropriate neuron\npermutation for each expert is missing to bring all of them in alignment. To\naddress this, we propose M-SMoE, which leverages routing statistics to guide\nexpert merging. Specifically, it starts with neuron permutation alignment for\nexperts; then, dominant experts and their \"group members\" are formed; lastly,\nevery expert group is merged into a single expert by utilizing each expert's\nactivation frequency as their weight for merging, thus diminishing the impact\nof insignificant experts. Moreover, we observed that our proposed merging\npromotes a low dimensionality in the merged expert's weight space, naturally\npaving the way for additional compression. Hence, our final method, MC-SMoE\n(i.e., Merge, then Compress SMoE), further decomposes the merged experts into\nlow-rank and structural sparse alternatives. Extensive experiments across 8\nbenchmarks validate the effectiveness of MC-SMoE. For instance, our MC-SMoE\nachieves up to 80% memory and a 20% FLOPs reduction, with virtually no loss in\nperformance.\n",
        "authors": "Pingzhi Li, Zhenyu Zhang, Prateek Yadav, Yi-Lin Sung, Yu Cheng, Mohit\n  Bansal, Tianlong Chen",
        "authors_parsed": [
            [
                "Li",
                "Pingzhi",
                ""
            ],
            [
                "Zhang",
                "Zhenyu",
                ""
            ],
            [
                "Yadav",
                "Prateek",
                ""
            ],
            [
                "Sung",
                "Yi-Lin",
                ""
            ],
            [
                "Cheng",
                "Yu",
                ""
            ],
            [
                "Bansal",
                "Mohit",
                ""
            ],
            [
                "Chen",
                "Tianlong",
                ""
            ]
        ],
        "categories": "cs.LG cs.AI cs.CL",
        "comments": "This paper is accepted in ICLR 2024",
        "doi": null,
        "id": "2310.01334",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Pingzhi Li",
        "title": "Merge, Then Compress: Demystify Efficient SMoE with Hints from Its\n  Routing Policy",
        "update_date": "2024-03-15",
        "versions": [
            {
                "created": "Mon, 2 Oct 2023 16:51:32 GMT",
                "version": "v1"
            },
            {
                "created": "Thu, 14 Mar 2024 11:01:15 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Do LMs infer the semantics of text from co-occurrence patterns in their\ntraining data? Merrill et al. (2022) argue that, in theory, probabilities\npredicted by an optimal LM encode semantic information about entailment\nrelations, but it is unclear whether neural LMs trained on corpora learn\nentailment in this way because of strong idealizing assumptions made by Merrill\net al. In this work, we investigate whether their theory can be used to decode\nentailment judgments from neural LMs. We find that a test similar to theirs can\ndecode entailment relations between natural sentences, well above random\nchance, though not perfectly, across many datasets and LMs. This suggests LMs\nimplicitly model aspects of semantics to predict semantic effects on sentence\nco-occurrence patterns. However, we find the test that predicts entailment in\npractice works in the opposite direction to the theoretical test. We thus\nrevisit the assumptions underlying the original test, finding its derivation\ndid not adequately account for redundancy in human-written text. We argue that\ncorrectly accounting for redundancy related to explanations might derive the\nobserved flipped test and, more generally, improve linguistic theories of human\nspeakers.\n",
        "authors": "William Merrill and Zhaofeng Wu and Norihito Naka and Yoon Kim and Tal\n  Linzen",
        "authors_parsed": [
            [
                "Merrill",
                "William",
                ""
            ],
            [
                "Wu",
                "Zhaofeng",
                ""
            ],
            [
                "Naka",
                "Norihito",
                ""
            ],
            [
                "Kim",
                "Yoon",
                ""
            ],
            [
                "Linzen",
                "Tal",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "Preprint",
        "doi": null,
        "id": "2402.13956",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "William Merrill",
        "title": "Can You Learn Semantics Through Next-Word Prediction? The Case of\n  Entailment",
        "update_date": "2024-03-04",
        "versions": [
            {
                "created": "Wed, 21 Feb 2024 17:36:07 GMT",
                "version": "v1"
            },
            {
                "created": "Thu, 29 Feb 2024 22:18:03 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Modular deep learning has been proposed for the efficient adaption of\npre-trained models to new tasks, domains and languages. In particular,\ncombining language adapters with task adapters has shown potential where no\nsupervised data exists for a language. In this paper, we explore the role of\nlanguage adapters in zero-shot cross-lingual transfer for natural language\nunderstanding (NLU) benchmarks. We study the effect of including a\ntarget-language adapter in detailed ablation studies with two multilingual\nmodels and three multilingual datasets. Our results show that the effect of\ntarget-language adapters is highly inconsistent across tasks, languages and\nmodels. Retaining the source-language adapter instead often leads to an\nequivalent, and sometimes to a better, performance. Removing the language\nadapter after training has only a weak negative effect, indicating that the\nlanguage adapters do not have a strong impact on the predictions.\n",
        "authors": "Jenny Kunz, Oskar Holmstr\\\"om",
        "authors_parsed": [
            [
                "Kunz",
                "Jenny",
                ""
            ],
            [
                "Holmstr\u00f6m",
                "Oskar",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": null,
        "doi": null,
        "id": "2402.00149",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Jenny Kunz",
        "title": "The Impact of Language Adapters in Cross-Lingual Transfer for NLU",
        "update_date": "2024-02-02",
        "versions": [
            {
                "created": "Wed, 31 Jan 2024 20:07:43 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  The meaning of polysemous words often varies in a highly productive yet\npredictable way. Generalizing the regularity between conventional senses to\nderive novel word meaning is crucial for automated processing of non-literal\nlanguage uses such as figurative expressions. We introduce a novel task called\nsystematic word meta-sense extension (SWORME) to test and improve language\nmodels' ability to extend word meaning to denote new semantic domains (also\ncalled meta-senses) that bear regular semantic relations with existing senses.\nWe found that language models prefer incremental lexical semantic change toward\nconceptually similar meta-senses such as logical metonymy, and are much worse\nat predicting highly non-literal meaning extensions such as metaphors. We\npropose a novel analogy-based method of word meaning extension, and show that\nit effectively improves language model systematicity in making both gradual and\nradical types of meta-sense extension. We further demonstrate that learning\nsystematic meta-sense extensions benefits language models on multiple\nbenchmarks of figurative language understanding.\n",
        "authors": "Lei Yu",
        "authors_parsed": [
            [
                "Yu",
                "Lei",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": null,
        "doi": null,
        "id": "2311.13029",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Lei Yu",
        "title": "Systematic word meta-sense extension",
        "update_date": "2023-11-23",
        "versions": [
            {
                "created": "Tue, 21 Nov 2023 22:30:37 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Despite the recent advancement in large language models (LLMs) and their high\nperformances across numerous benchmarks, recent research has unveiled that LLMs\nsuffer from hallucinations and unfaithful reasoning. This work studies a\nspecific type of hallucination induced by semantic associations. Specifically,\nwe investigate to what extent LLMs take shortcuts from certain keyword/entity\nbiases in the prompt instead of following the correct reasoning path. To\nquantify this phenomenon, we propose a novel probing method and benchmark\ncalled EureQA. We start from questions that LLMs will answer correctly with\nutmost certainty, and mask the important entity with evidence sentence\nrecursively, asking models to find masked entities according to a chain of\nevidence before answering the question.\n  During the construction of the evidence, we purposefully replace semantic\nclues (entities) that may lead to the correct answer with distractor clues\n(evidence) that will not directly lead to the correct answer but require a\nchain-like reasoning process. We evaluate if models can follow the correct\nreasoning chain instead of short-cutting through distractor clues. We find that\nexisting LLMs lack the necessary capabilities to follow correct reasoning paths\nand resist the attempt of greedy shortcuts. We show that the distractor\nsemantic associations often lead to model hallucination, which is strong\nevidence that questions the validity of current LLM reasoning.\n",
        "authors": "Bangzheng Li, Ben Zhou, Fei Wang, Xingyu Fu, Dan Roth, Muhao Chen",
        "authors_parsed": [
            [
                "Li",
                "Bangzheng",
                ""
            ],
            [
                "Zhou",
                "Ben",
                ""
            ],
            [
                "Wang",
                "Fei",
                ""
            ],
            [
                "Fu",
                "Xingyu",
                ""
            ],
            [
                "Roth",
                "Dan",
                ""
            ],
            [
                "Chen",
                "Muhao",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI",
        "comments": "Work in progress",
        "doi": null,
        "id": "2311.09702",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Bangzheng Li",
        "title": "Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go\n  without Hallucination?",
        "update_date": "2024-03-14",
        "versions": [
            {
                "created": "Thu, 16 Nov 2023 09:27:36 GMT",
                "version": "v1"
            },
            {
                "created": "Wed, 13 Mar 2024 09:11:15 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  The decoder-only Transformer architecture with causal masking and relative\nposition encoding (RPE) has become the de facto choice in language modeling.\nDespite its exceptional performance across various tasks, we have identified\ntwo limitations: First, it requires all attention scores to be non-zero and sum\nup to 1, even if the current embedding has sufficient self-contained\ninformation. This compels the model to assign disproportional excessive\nattention to specific tokens. Second, RPE-based Transformers are not universal\napproximators due to their limited capacity at encoding absolute positional\ninformation, which limits their application in position-critical tasks. In this\nwork, we propose StableMask: a parameter-free method to address both\nlimitations by refining the causal mask. It introduces pseudo-attention values\nto balance attention distributions and encodes absolute positional information\nvia a progressively decreasing mask ratio. StableMask's effectiveness is\nvalidated both theoretically and empirically, showing significant enhancements\nin language models with parameter sizes ranging from 71M to 1.4B across diverse\ndatasets and encoding methods. We further show that it naturally supports (1)\nefficient extrapolation without special tricks such as StreamingLLM and (2)\neasy integration with existing attention optimization techniques.\n",
        "authors": "Qingyu Yin, Xuzheng He, Xiang Zhuang, Yu Zhao, Jianhua Yao, Xiaoyu\n  Shen, Qiang Zhang",
        "authors_parsed": [
            [
                "Yin",
                "Qingyu",
                ""
            ],
            [
                "He",
                "Xuzheng",
                ""
            ],
            [
                "Zhuang",
                "Xiang",
                ""
            ],
            [
                "Zhao",
                "Yu",
                ""
            ],
            [
                "Yao",
                "Jianhua",
                ""
            ],
            [
                "Shen",
                "Xiaoyu",
                ""
            ],
            [
                "Zhang",
                "Qiang",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI",
        "comments": "Preprint",
        "doi": null,
        "id": "2402.04779",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Qingyu Yin",
        "title": "StableMask: Refining Causal Masking in Decoder-only Transformer",
        "update_date": "2024-02-08",
        "versions": [
            {
                "created": "Wed, 7 Feb 2024 12:01:02 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Large language models (LLMs) such as GPT-4 have recently demonstrated\nimpressive results across a wide range of tasks. LLMs are still limited,\nhowever, in that they frequently fail at complex reasoning, their reasoning\nprocesses are opaque, they are prone to 'hallucinate' facts, and there are\nconcerns about their underlying biases. Letting models verbalize reasoning\nsteps as natural language, a technique known as chain-of-thought prompting, has\nrecently been proposed as a way to address some of these issues. Here we\npresent ThoughtSource, a meta-dataset and software library for chain-of-thought\n(CoT) reasoning. The goal of ThoughtSource is to improve future artificial\nintelligence systems by facilitating qualitative understanding of CoTs,\nenabling empirical evaluations, and providing training data. This first release\nof ThoughtSource integrates seven scientific/medical, three general-domain and\nfive math word question answering datasets.\n",
        "authors": "Simon Ott, Konstantin Hebenstreit, Valentin Li\\'evin, Christoffer\n  Egeberg Hother, Milad Moradi, Maximilian Mayrhauser, Robert Praas, Ole\n  Winther, Matthias Samwald",
        "authors_parsed": [
            [
                "Ott",
                "Simon",
                ""
            ],
            [
                "Hebenstreit",
                "Konstantin",
                ""
            ],
            [
                "Li\u00e9vin",
                "Valentin",
                ""
            ],
            [
                "Hother",
                "Christoffer Egeberg",
                ""
            ],
            [
                "Moradi",
                "Milad",
                ""
            ],
            [
                "Mayrhauser",
                "Maximilian",
                ""
            ],
            [
                "Praas",
                "Robert",
                ""
            ],
            [
                "Winther",
                "Ole",
                ""
            ],
            [
                "Samwald",
                "Matthias",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI",
        "comments": "Revision: added datasets, formatting",
        "doi": "10.1038/s41597-023-02433-3",
        "id": "2301.11596",
        "journal-ref": "Scientific Data 10, 528 (2023)",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Matthias Samwald",
        "title": "ThoughtSource: A central hub for large language model reasoning data",
        "update_date": "2024-02-27",
        "versions": [
            {
                "created": "Fri, 27 Jan 2023 08:45:53 GMT",
                "version": "v1"
            },
            {
                "created": "Tue, 28 Feb 2023 11:10:43 GMT",
                "version": "v2"
            },
            {
                "created": "Wed, 19 Jul 2023 15:25:37 GMT",
                "version": "v3"
            },
            {
                "created": "Thu, 20 Jul 2023 08:58:12 GMT",
                "version": "v4"
            },
            {
                "created": "Thu, 27 Jul 2023 09:37:35 GMT",
                "version": "v5"
            }
        ]
    },
    {
        "abstract": "  We present MedCATTrainer an interface for building, improving and customising\na given Named Entity Recognition and Linking (NER+L) model for biomedical\ndomain text. NER+L is often used as a first step in deriving value from\nclinical text. Collecting labelled data for training models is difficult due to\nthe need for specialist domain knowledge. MedCATTrainer offers an interactive\nweb-interface to inspect and improve recognised entities from an underlying\nNER+L model via active learning. Secondary use of data for clinical research\noften has task and context specific criteria. MedCATTrainer provides a further\ninterface to define and collect supervised learning training data for\nresearcher specific use cases. Initial results suggest our approach allows for\nefficient and accurate collection of research use case specific training data.\n",
        "authors": "Thomas Searle, Zeljko Kraljevic, Rebecca Bendayan, Daniel Bean,\n  Richard Dobson",
        "authors_parsed": [
            [
                "Searle",
                "Thomas",
                ""
            ],
            [
                "Kraljevic",
                "Zeljko",
                ""
            ],
            [
                "Bendayan",
                "Rebecca",
                ""
            ],
            [
                "Bean",
                "Daniel",
                ""
            ],
            [
                "Dobson",
                "Richard",
                ""
            ]
        ],
        "categories": "cs.HC cs.CL cs.LG",
        "comments": null,
        "doi": "10.18653/v1/D19-3024",
        "id": "1907.07322",
        "journal-ref": "EMNLP/IJCNLP 2019",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Thomas Searle",
        "title": "MedCATTrainer: A Biomedical Free Text Annotation Interface with Active\n  Learning and Research Use Case Specific Customisation",
        "update_date": "2023-02-28",
        "versions": [
            {
                "created": "Tue, 16 Jul 2019 15:32:04 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Transformers have been shown to work well for the task of English euphemism\ndisambiguation, in which a potentially euphemistic term (PET) is classified as\neuphemistic or non-euphemistic in a particular context. In this study, we\nexpand on the task in two ways. First, we annotate PETs for vagueness, a\nlinguistic property associated with euphemisms, and find that transformers are\ngenerally better at classifying vague PETs, suggesting linguistic differences\nin the data that impact performance. Second, we present novel euphemism corpora\nin three different languages: Yoruba, Spanish, and Mandarin Chinese. We perform\neuphemism disambiguation experiments in each language using multilingual\ntransformer models mBERT and XLM-RoBERTa, establishing preliminary results from\nwhich to launch future work.\n",
        "authors": "Patrick Lee, Iyanuoluwa Shode, Alain Chirino Trujillo, Yuan Zhao,\n  Olumide Ebenezer Ojo, Diana Cuevas Plancarte, Anna Feldman, Jing Peng",
        "authors_parsed": [
            [
                "Lee",
                "Patrick",
                ""
            ],
            [
                "Shode",
                "Iyanuoluwa",
                ""
            ],
            [
                "Trujillo",
                "Alain Chirino",
                ""
            ],
            [
                "Zhao",
                "Yuan",
                ""
            ],
            [
                "Ojo",
                "Olumide Ebenezer",
                ""
            ],
            [
                "Plancarte",
                "Diana Cuevas",
                ""
            ],
            [
                "Feldman",
                "Anna",
                ""
            ],
            [
                "Peng",
                "Jing",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": null,
        "doi": null,
        "id": "2306.00217",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Olumide Ebenezer Ojo",
        "title": "FEED PETs: Further Experimentation and Expansion on the Disambiguation\n  of Potentially Euphemistic Terms",
        "update_date": "2023-06-08",
        "versions": [
            {
                "created": "Wed, 31 May 2023 22:23:20 GMT",
                "version": "v1"
            },
            {
                "created": "Tue, 6 Jun 2023 19:17:14 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Data visualization is a powerful tool for exploring and communicating\ninsights in various domains. To automate visualization choice for datasets, a\ntask known as visualization recommendation has been proposed. Various\nmachine-learning-based approaches have been developed for this purpose, but\nthey often require a large corpus of dataset-visualization pairs for training\nand lack natural explanations for their results. To address this research gap,\nwe propose LLM4Vis, a novel ChatGPT-based prompting approach to perform\nvisualization recommendation and return human-like explanations using very few\ndemonstration examples. Our approach involves feature description,\ndemonstration example selection, explanation generation, demonstration example\nconstruction, and inference steps. To obtain demonstration examples with\nhigh-quality explanations, we propose a new explanation generation\nbootstrapping to iteratively refine generated explanations by considering the\nprevious generation and template-based hint. Evaluations on the VizML dataset\nshow that LLM4Vis outperforms or performs similarly to supervised learning\nmodels like Random Forest, Decision Tree, and MLP in both few-shot and\nzero-shot settings. The qualitative evaluation also shows the effectiveness of\nexplanations generated by LLM4Vis. We make our code publicly available at\n\\href{https://github.com/demoleiwang/LLM4Vis}{https://github.com/demoleiwang/LLM4Vis}.\n",
        "authors": "Lei Wang, Songheng Zhang, Yun Wang, Ee-Peng Lim, Yong Wang",
        "authors_parsed": [
            [
                "Wang",
                "Lei",
                ""
            ],
            [
                "Zhang",
                "Songheng",
                ""
            ],
            [
                "Wang",
                "Yun",
                ""
            ],
            [
                "Lim",
                "Ee-Peng",
                ""
            ],
            [
                "Wang",
                "Yong",
                ""
            ]
        ],
        "categories": "cs.HC cs.CL",
        "comments": "EMNLP 2023 (Industry Track)",
        "doi": null,
        "id": "2310.07652",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Lei Wang",
        "title": "LLM4Vis: Explainable Visualization Recommendation using ChatGPT",
        "update_date": "2023-10-17",
        "versions": [
            {
                "created": "Wed, 11 Oct 2023 16:51:46 GMT",
                "version": "v1"
            },
            {
                "created": "Mon, 16 Oct 2023 03:34:47 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  We investigate the effectiveness of using a large ensemble of advanced neural\nlanguage models (NLMs) for lattice rescoring on automatic speech recognition\n(ASR) hypotheses. Previous studies have reported the effectiveness of combining\na small number of NLMs. In contrast, in this study, we combine up to eight\nNLMs, i.e., forward/backward long short-term memory/Transformer-LMs that are\ntrained with two different random initialization seeds. We combine these NLMs\nthrough iterative lattice generation. Since these NLMs work complementarily\nwith each other, by combining them one by one at each rescoring iteration,\nlanguage scores attached to given lattice arcs can be gradually refined.\nConsequently, errors of the ASR hypotheses can be gradually reduced. We also\ninvestigate the effectiveness of carrying over contextual information (previous\nrescoring results) across a lattice sequence of a long speech such as a lecture\nspeech. In experiments using a lecture speech corpus, by combining the eight\nNLMs and using context carry-over, we obtained a 24.4% relative word error rate\nreduction from the ASR 1-best baseline. For further comparison, we performed\nsimultaneous (i.e., non-iterative) NLM combination and 100-best rescoring using\nthe large ensemble of NLMs, which confirmed the advantage of lattice rescoring\nwith iterative NLM combination.\n",
        "authors": "Atsunori Ogawa, Naohiro Tawara, Marc Delcroix, Shoko Araki",
        "authors_parsed": [
            [
                "Ogawa",
                "Atsunori",
                ""
            ],
            [
                "Tawara",
                "Naohiro",
                ""
            ],
            [
                "Delcroix",
                "Marc",
                ""
            ],
            [
                "Araki",
                "Shoko",
                ""
            ]
        ],
        "categories": "eess.AS cs.CL cs.SD",
        "comments": "Accepted to ICASSP 2022",
        "doi": null,
        "id": "2312.12764",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Atsunori Ogawa",
        "title": "Lattice Rescoring Based on Large Ensemble of Complementary Neural\n  Language Models",
        "update_date": "2023-12-21",
        "versions": [
            {
                "created": "Wed, 20 Dec 2023 04:52:24 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  We introduce TableLLM, a robust large language model (LLM) with 13 billion\nparameters, purpose-built for proficiently handling tabular data manipulation\ntasks, whether they are embedded within documents or spreadsheets, catering to\nreal-world office scenarios. We propose a distant supervision method for\ntraining, which comprises a reasoning process extension strategy, aiding in\ntraining LLMs to understand reasoning patterns more effectively as well as a\ncross-way validation strategy, ensuring the quality of the automatically\ngenerated data. To evaluate the performance of TableLLM, we have crafted a\nbenchmark tailored to address both document and spreadsheet formats as well as\nconstructed a well-organized evaluation pipeline capable of handling both\nscenarios. Thorough evaluations underscore the advantages of TableLLM when\ncompared to various existing general-purpose and tabular data-focused LLMs. We\nhave publicly released the model checkpoint, source code, benchmarks, and a web\napplication for user interaction.Our codes and data are publicly available at\nhttps://github.com/TableLLM/TableLLM.\n",
        "authors": "Xiaokang Zhang, Jing Zhang, Zeyao Ma, Yang Li, Bohan Zhang, Guanlin\n  Li, Zijun Yao, Kangli Xu, Jinchang Zhou, Daniel Zhang-Li, Jifan Yu, Shu Zhao,\n  Juanzi Li, Jie Tang",
        "authors_parsed": [
            [
                "Zhang",
                "Xiaokang",
                ""
            ],
            [
                "Zhang",
                "Jing",
                ""
            ],
            [
                "Ma",
                "Zeyao",
                ""
            ],
            [
                "Li",
                "Yang",
                ""
            ],
            [
                "Zhang",
                "Bohan",
                ""
            ],
            [
                "Li",
                "Guanlin",
                ""
            ],
            [
                "Yao",
                "Zijun",
                ""
            ],
            [
                "Xu",
                "Kangli",
                ""
            ],
            [
                "Zhou",
                "Jinchang",
                ""
            ],
            [
                "Zhang-Li",
                "Daniel",
                ""
            ],
            [
                "Yu",
                "Jifan",
                ""
            ],
            [
                "Zhao",
                "Shu",
                ""
            ],
            [
                "Li",
                "Juanzi",
                ""
            ],
            [
                "Tang",
                "Jie",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "https://tablellm.github.io/",
        "doi": null,
        "id": "2403.19318",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Bohan Zhang",
        "title": "TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office\n  Usage Scenarios",
        "update_date": "2024-04-02",
        "versions": [
            {
                "created": "Thu, 28 Mar 2024 11:21:12 GMT",
                "version": "v1"
            },
            {
                "created": "Mon, 1 Apr 2024 05:10:56 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Code-Switching (CS) is referred to the phenomenon of alternately using words\nand phrases from different languages. While today's neural end-to-end (E2E)\nmodels deliver state-of-the-art performances on the task of automatic speech\nrecognition (ASR) it is commonly known that these systems are very\ndata-intensive. However, there is only a few transcribed and aligned CS speech\navailable. To overcome this problem and train multilingual systems which can\ntranscribe CS speech, we propose a simple yet effective data augmentation in\nwhich audio and corresponding labels of different source languages are\nconcatenated. By using this training data, our E2E model improves on\ntranscribing CS speech. It also surpasses monolingual models on monolingual\ntests. The results show that this augmentation technique can even improve the\nmodel's performance on inter-sentential language switches not seen during\ntraining by 5,03% WER.\n",
        "authors": "Enes Yavuz Ugan, Christian Huber, Juan Hussain and Alexander Waibel",
        "authors_parsed": [
            [
                "Ugan",
                "Enes Yavuz",
                ""
            ],
            [
                "Huber",
                "Christian",
                ""
            ],
            [
                "Hussain",
                "Juan",
                ""
            ],
            [
                "Waibel",
                "Alexander",
                ""
            ]
        ],
        "categories": "cs.CL cs.SD eess.AS",
        "comments": "18 pages",
        "doi": null,
        "id": "2210.08992",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "report-no": null,
        "submitter": "Enes Yavuz Ugan",
        "title": "Language-agnostic Code-Switching in Sequence-To-Sequence Speech\n  Recognition",
        "update_date": "2023-07-04",
        "versions": [
            {
                "created": "Mon, 17 Oct 2022 12:15:57 GMT",
                "version": "v1"
            },
            {
                "created": "Mon, 3 Jul 2023 10:01:47 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Knowledge Graphs (KG) provide us with a structured, flexible, transparent,\ncross-system, and collaborative way of organizing our knowledge and data across\nvarious domains in society and industrial as well as scientific disciplines.\nKGs surpass any other form of representation in terms of effectiveness.\nHowever, Knowledge Graph Engineering (KGE) requires in-depth experiences of\ngraph structures, web technologies, existing models and vocabularies, rule\nsets, logic, as well as best practices. It also demands a significant amount of\nwork. Considering the advancements in large language models (LLMs) and their\ninterfaces and applications in recent years, we have conducted comprehensive\nexperiments with ChatGPT to explore its potential in supporting KGE. In this\npaper, we present a selection of these experiments and their results to\ndemonstrate how ChatGPT can assist us in the development and management of KGs.\n",
        "authors": "Lars-Peter Meyer, Claus Stadler, Johannes Frey, Norman Radtke, Kurt\n  Junghanns, Roy Meissner, Gordian Dziwis, Kirill Bulert, Michael Martin",
        "authors_parsed": [
            [
                "Meyer",
                "Lars-Peter",
                ""
            ],
            [
                "Stadler",
                "Claus",
                ""
            ],
            [
                "Frey",
                "Johannes",
                ""
            ],
            [
                "Radtke",
                "Norman",
                ""
            ],
            [
                "Junghanns",
                "Kurt",
                ""
            ],
            [
                "Meissner",
                "Roy",
                ""
            ],
            [
                "Dziwis",
                "Gordian",
                ""
            ],
            [
                "Bulert",
                "Kirill",
                ""
            ],
            [
                "Martin",
                "Michael",
                ""
            ]
        ],
        "categories": "cs.AI cs.CL cs.DB",
        "comments": "to appear in conference proceedings of AI-Tomorrow-23, 29.+30.6.2023\n  in Leipzig, Germany",
        "doi": null,
        "id": "2307.06917",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Lars-Peter Meyer",
        "title": "LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT",
        "update_date": "2023-07-14",
        "versions": [
            {
                "created": "Thu, 13 Jul 2023 17:31:41 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Current developments in large language models (LLMs) have enabled impressive\nzero-shot capabilities across various natural language tasks. An interesting\napplication of these systems is in the automated assessment of natural language\ngeneration (NLG), a highly challenging area with great practical benefit. In\nthis paper, we explore two options for exploiting the emergent abilities of\nLLMs for zero-shot NLG assessment: absolute score prediction, and comparative\nassessment which uses relative comparisons between pairs of candidates. Though\ncomparative assessment has not been extensively studied in NLG assessment, we\nnote that humans often find it more intuitive to compare two options rather\nthan scoring each one independently. This work examines comparative assessment\nfrom multiple perspectives: performance compared to absolute grading;\npositional biases in the prompt; and efficient ranking in terms of the number\nof comparisons. We illustrate that LLM comparative assessment is a simple,\ngeneral and effective approach for NLG assessment. For moderate-sized\nopen-source LLMs, such as FlanT5 and Llama2-chat, comparative assessment is\nsuperior to prompt scoring, and in many cases can achieve performance\ncompetitive with state-of-the-art methods. Additionally, we demonstrate that\nLLMs often exhibit strong positional biases when making pairwise comparisons,\nand we propose debiasing methods that can further improve performance.\n",
        "authors": "Adian Liusie, Potsawee Manakul, Mark J. F. Gales",
        "authors_parsed": [
            [
                "Liusie",
                "Adian",
                ""
            ],
            [
                "Manakul",
                "Potsawee",
                ""
            ],
            [
                "Gales",
                "Mark J. F.",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "To Appear at EACL 2024",
        "doi": null,
        "id": "2307.07889",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Adian Liusie",
        "title": "LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise\n  Comparisons using Large Language Models",
        "update_date": "2024-02-07",
        "versions": [
            {
                "created": "Sat, 15 Jul 2023 22:02:12 GMT",
                "version": "v1"
            },
            {
                "created": "Wed, 16 Aug 2023 14:55:35 GMT",
                "version": "v2"
            },
            {
                "created": "Tue, 6 Feb 2024 17:05:58 GMT",
                "version": "v3"
            }
        ]
    },
    {
        "abstract": "  Teachers and students are increasingly relying on online learning resources\nto supplement the ones provided in school. This increase in the breadth and\ndepth of available resources is a great thing for students, but only provided\nthey are able to find answers to their queries. Question-answering and\ninformation retrieval systems have benefited from public datasets to train and\nevaluate their algorithms, but most of these datasets have been in English text\nwritten by and for adults. We introduce a new public French question-answering\ndataset collected from Alloprof, a Quebec-based primary and high-school help\nwebsite, containing 29 349 questions and their explanations in a variety of\nschool subjects from 10 368 students, with more than half of the explanations\ncontaining links to other questions or some of the 2 596 reference pages on the\nwebsite. We also present a case study of this dataset in an information\nretrieval task. This dataset was collected on the Alloprof public forum, with\nall questions verified for their appropriateness and the explanations verified\nboth for their appropriateness and their relevance to the question. To predict\nrelevant documents, architectures using pre-trained BERT models were fine-tuned\nand evaluated. This dataset will allow researchers to develop\nquestion-answering, information retrieval and other algorithms specifically for\nthe French speaking education context. Furthermore, the range of language\nproficiency, images, mathematical symbols and spelling mistakes will\nnecessitate algorithms based on a multimodal comprehension. The case study we\npresent as a baseline shows an approach that relies on recent techniques\nprovides an acceptable performance level, but more work is necessary before it\ncan reliably be used and trusted in a production setting.\n",
        "authors": "Antoine Lefebvre-Brossard, Stephane Gazaille, Michel C. Desmarais",
        "authors_parsed": [
            [
                "Lefebvre-Brossard",
                "Antoine",
                ""
            ],
            [
                "Gazaille",
                "Stephane",
                ""
            ],
            [
                "Desmarais",
                "Michel C.",
                ""
            ]
        ],
        "categories": "cs.CL cs.IR cs.LG",
        "comments": null,
        "doi": null,
        "id": "2302.07738",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "report-no": null,
        "submitter": "Antoine Lefebvre-Brossard",
        "title": "Alloprof: a new French question-answer education dataset and its use in\n  an information retrieval case study",
        "update_date": "2023-04-17",
        "versions": [
            {
                "created": "Fri, 10 Feb 2023 20:23:27 GMT",
                "version": "v1"
            },
            {
                "created": "Fri, 14 Apr 2023 13:20:07 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Large Language Models pre-trained with self-supervised learning have\ndemonstrated impressive zero-shot generalization capabilities on a wide\nspectrum of tasks. In this work, we present WeLM: a well-read pre-trained\nlanguage model for Chinese that is able to seamlessly perform different types\nof tasks with zero or few-shot demonstrations. WeLM is trained with 10B\nparameters by \"reading\" a curated high-quality corpus covering a wide range of\ntopics. We show that WeLM is equipped with broad knowledge on various domains\nand languages. On 18 monolingual (Chinese) tasks, WeLM can significantly\noutperform existing pre-trained models with similar sizes and match the\nperformance of models up to 25 times larger. WeLM also exhibits strong\ncapabilities in multi-lingual and code-switching understanding, outperforming\nexisting multilingual language models pre-trained on 30 languages. Furthermore,\nWe collected human-written prompts for a large set of supervised datasets in\nChinese and fine-tuned WeLM with multi-prompted training. The resulting model\ncan attain strong generalization on unseen types of tasks and outperform the\nunsupervised WeLM in zero-shot learning. Finally, we demonstrate that WeLM has\nbasic skills at explaining and calibrating the decisions from itself, which can\nbe promising directions for future research. Our models can be applied from\nhttps://welm.weixin.qq.com/docs/api/.\n",
        "authors": "Hui Su, Xiao Zhou, Houjin Yu, Xiaoyu Shen, Yuwen Chen, Zilin Zhu, Yang\n  Yu, Jie Zhou",
        "authors_parsed": [
            [
                "Su",
                "Hui",
                ""
            ],
            [
                "Zhou",
                "Xiao",
                ""
            ],
            [
                "Yu",
                "Houjin",
                ""
            ],
            [
                "Shen",
                "Xiaoyu",
                ""
            ],
            [
                "Chen",
                "Yuwen",
                ""
            ],
            [
                "Zhu",
                "Zilin",
                ""
            ],
            [
                "Yu",
                "Yang",
                ""
            ],
            [
                "Zhou",
                "Jie",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI",
        "comments": null,
        "doi": null,
        "id": "2209.10372",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Hui Su",
        "title": "WeLM: A Well-Read Pre-trained Language Model for Chinese",
        "update_date": "2023-05-17",
        "versions": [
            {
                "created": "Wed, 21 Sep 2022 14:05:30 GMT",
                "version": "v1"
            },
            {
                "created": "Thu, 22 Sep 2022 02:52:04 GMT",
                "version": "v2"
            },
            {
                "created": "Tue, 27 Sep 2022 06:17:14 GMT",
                "version": "v3"
            },
            {
                "created": "Wed, 12 Oct 2022 13:29:57 GMT",
                "version": "v4"
            },
            {
                "created": "Tue, 16 May 2023 03:00:22 GMT",
                "version": "v5"
            }
        ]
    },
    {
        "abstract": "  Recently, various intermediate layer distillation (ILD) objectives have been\nshown to improve compression of BERT models via Knowledge Distillation (KD).\nHowever, a comprehensive evaluation of the objectives in both task-specific and\ntask-agnostic settings is lacking. To the best of our knowledge, this is the\nfirst work comprehensively evaluating distillation objectives in both settings.\nWe show that attention transfer gives the best performance overall. We also\nstudy the impact of layer choice when initializing the student from the teacher\nlayers, finding a significant impact on the performance in task-specific\ndistillation. For vanilla KD and hidden states transfer, initialisation with\nlower layers of the teacher gives a considerable improvement over higher\nlayers, especially on the task of QNLI (up to an absolute percentage change of\n17.8 in accuracy). Attention transfer behaves consistently under different\ninitialisation settings. We release our code as an efficient transformer-based\nmodel distillation framework for further studies.\n",
        "authors": "Xinpeng Wang, Leonie Weissweiler, Hinrich Sch\\\"utze, Barbara Plank",
        "authors_parsed": [
            [
                "Wang",
                "Xinpeng",
                ""
            ],
            [
                "Weissweiler",
                "Leonie",
                ""
            ],
            [
                "Sch\u00fctze",
                "Hinrich",
                ""
            ],
            [
                "Plank",
                "Barbara",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI cs.LG",
        "comments": "ACL 2023",
        "doi": null,
        "id": "2305.15032",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Xinpeng Wang",
        "title": "How to Distill your BERT: An Empirical Study on the Impact of Weight\n  Initialisation and Distillation Objectives",
        "update_date": "2023-05-25",
        "versions": [
            {
                "created": "Wed, 24 May 2023 11:16:09 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features. While these maps allow for the prediction\nof point-wise saliency maps when queried for a certain language concept,\nlarge-scale environments and abstract queries beyond the object level still\npose a considerable hurdle, ultimately limiting language-grounded robotic\nnavigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D\nscene graph mapping approach for language-grounded robot navigation. Leveraging\nopen-vocabulary vision foundation models, we first obtain state-of-the-art\nopen-vocabulary segment-level maps in 3D and subsequently construct a 3D scene\ngraph hierarchy consisting of floor, room, and object concepts, each enriched\nwith open-vocabulary features. Our approach is able to represent multi-story\nbuildings and allows robotic traversal of those using a cross-floor Voronoi\ngraph. HOV-SG is evaluated on three distinct datasets and surpasses previous\nbaselines in open-vocabulary semantic accuracy on the object, room, and floor\nlevel while producing a 75% reduction in representation size compared to dense\nopen-vocabulary maps. In order to prove the efficacy and generalization\ncapabilities of HOV-SG, we showcase successful long-horizon\nlanguage-conditioned robot navigation within real-world multi-storage\nenvironments. We provide code and trial video data at http://hovsg.github.io/.\n",
        "authors": "Abdelrhman Werby, Chenguang Huang, Martin B\\\"uchner, Abhinav Valada,\n  Wolfram Burgard",
        "authors_parsed": [
            [
                "Werby",
                "Abdelrhman",
                ""
            ],
            [
                "Huang",
                "Chenguang",
                ""
            ],
            [
                "B\u00fcchner",
                "Martin",
                ""
            ],
            [
                "Valada",
                "Abhinav",
                ""
            ],
            [
                "Burgard",
                "Wolfram",
                ""
            ]
        ],
        "categories": "cs.RO cs.AI cs.CL cs.CV cs.LG",
        "comments": "Code and video are available at http://hovsg.github.io/",
        "doi": null,
        "id": "2403.17846",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Chenguang Huang",
        "title": "Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot\n  Navigation",
        "update_date": "2024-03-27",
        "versions": [
            {
                "created": "Tue, 26 Mar 2024 16:36:43 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  While large language models (LLMs) have proven to be effective on a large\nvariety of tasks, they are also known to hallucinate information. To measure\nwhether an LLM prefers factually consistent continuations of its input, we\npropose a new benchmark called FIB(Factual Inconsistency Benchmark) that\nfocuses on the task of summarization. Specifically, our benchmark involves\ncomparing the scores an LLM assigns to a factually consistent versus a\nfactually inconsistent summary for an input news article. For factually\nconsistent summaries, we use human-written reference summaries that we manually\nverify as factually consistent. To generate summaries that are factually\ninconsistent, we generate summaries from a suite of summarization models that\nwe have manually annotated as factually inconsistent. A model's factual\nconsistency is then measured according to its accuracy, i.e.\\ the proportion of\ndocuments where it assigns a higher score to the factually consistent summary.\nTo validate the usefulness of FIB, we evaluate 23 large language models ranging\nfrom 1B to 176B parameters from six different model families including BLOOM\nand OPT. We find that existing LLMs generally assign a higher score to\nfactually consistent summaries than to factually inconsistent summaries.\nHowever, if the factually inconsistent summaries occur verbatim in the\ndocument, then LLMs assign a higher score to these factually inconsistent\nsummaries than factually consistent summaries. We validate design choices in\nour benchmark including the scoring method and source of distractor summaries.\nOur code and benchmark data can be found at https://github.com/r-three/fib.\n",
        "authors": "Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal,\n  Colin Raffel",
        "authors_parsed": [
            [
                "Tam",
                "Derek",
                ""
            ],
            [
                "Mascarenhas",
                "Anisha",
                ""
            ],
            [
                "Zhang",
                "Shiyue",
                ""
            ],
            [
                "Kwan",
                "Sarah",
                ""
            ],
            [
                "Bansal",
                "Mohit",
                ""
            ],
            [
                "Raffel",
                "Colin",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": null,
        "doi": null,
        "id": "2211.08412",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Derek Tam",
        "title": "Evaluating the Factual Consistency of Large Language Models Through News\n  Summarization",
        "update_date": "2023-12-05",
        "versions": [
            {
                "created": "Tue, 15 Nov 2022 18:50:34 GMT",
                "version": "v1"
            },
            {
                "created": "Sat, 2 Dec 2023 18:10:15 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Recent approaches to text analysis from social media and other corpora rely\non word lists to detect topics, measure meaning, or to select relevant\ndocuments. These lists are often generated by applying computational lexicon\nexpansion methods to small, manually-curated sets of root words. Despite the\nwide use of this approach, we still lack an exhaustive comparative analysis of\nthe performance of lexicon expansion methods and how they can be improved with\nadditional linguistic data. In this work, we present LEXpander, a method for\nlexicon expansion that leverages novel data on colexification, i.e. semantic\nnetworks connecting words based on shared concepts and translations to other\nlanguages. We evaluate LEXpander in a benchmark including widely used methods\nfor lexicon expansion based on various word embedding models and synonym\nnetworks. We find that LEXpander outperforms existing approaches in terms of\nboth precision and the trade-off between precision and recall of generated word\nlists in a variety of tests. Our benchmark includes several linguistic\ncategories and sentiment variables in English and German. We also show that the\nexpanded word lists constitute a high-performing text analysis method in\napplication cases to various corpora. This way, LEXpander poses a systematic\nautomated solution to expand short lists of words into exhaustive and accurate\nword lists that can closely approximate word lists generated by experts in\npsychology and linguistics.\n",
        "authors": "Anna Di Natale and David Garcia",
        "authors_parsed": [
            [
                "Di Natale",
                "Anna",
                ""
            ],
            [
                "Garcia",
                "David",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "41 pages, 5 figures",
        "doi": "10.3758/s13428-023-02063-y",
        "id": "2205.15850",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Anna Di Natale",
        "title": "LEXpander: applying colexification networks to automated lexicon\n  expansion",
        "update_date": "2023-03-14",
        "versions": [
            {
                "created": "Tue, 31 May 2022 14:55:29 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Recent advances in large language models (LLMs), such as ChatGPT, have\nshowcased remarkable zero-shot performance across various NLP tasks. However,\nthe potential of LLMs in personality detection, which involves identifying an\nindividual's personality from their written texts, remains largely unexplored.\nDrawing inspiration from Psychological Questionnaires, which are carefully\ndesigned by psychologists to evaluate individual personality traits through a\nseries of targeted items, we argue that these items can be regarded as a\ncollection of well-structured chain-of-thought (CoT) processes. By\nincorporating these processes, LLMs can enhance their capabilities to make more\nreasonable inferences on personality from textual input. In light of this, we\npropose a novel personality detection method, called PsyCoT, which mimics the\nway individuals complete psychological questionnaires in a multi-turn dialogue\nmanner. In particular, we employ a LLM as an AI assistant with a specialization\nin text analysis. We prompt the assistant to rate individual items at each turn\nand leverage the historical rating results to derive a conclusive personality\npreference. Our experiments demonstrate that PsyCoT significantly improves the\nperformance and robustness of GPT-3.5 in personality detection, achieving an\naverage F1 score improvement of 4.23/10.63 points on two benchmark datasets\ncompared to the standard prompting method. Our code is available at\nhttps://github.com/TaoYang225/PsyCoT.\n",
        "authors": "Tao Yang, Tianyuan Shi, Fanqi Wan, Xiaojun Quan, Qifan Wang, Bingzhe\n  Wu, Jiaxiang Wu",
        "authors_parsed": [
            [
                "Yang",
                "Tao",
                ""
            ],
            [
                "Shi",
                "Tianyuan",
                ""
            ],
            [
                "Wan",
                "Fanqi",
                ""
            ],
            [
                "Quan",
                "Xiaojun",
                ""
            ],
            [
                "Wang",
                "Qifan",
                ""
            ],
            [
                "Wu",
                "Bingzhe",
                ""
            ],
            [
                "Wu",
                "Jiaxiang",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "Accepted to Findings of EMNLP 2023",
        "doi": null,
        "id": "2310.20256",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Tao Yang",
        "title": "PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for\n  Personality Detection",
        "update_date": "2023-11-07",
        "versions": [
            {
                "created": "Tue, 31 Oct 2023 08:23:33 GMT",
                "version": "v1"
            },
            {
                "created": "Sun, 5 Nov 2023 03:19:18 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  We propose a voting-driven semi-supervised approach to automatically acquire\nthe typical duration of an event and use it as pseudo-labeled data. The human\nevaluation demonstrates that our pseudo labels exhibit surprisingly high\naccuracy and balanced coverage. In the temporal commonsense QA task,\nexperimental results show that using only pseudo examples of 400 events, we\nachieve performance comparable to the existing BERT-based weakly supervised\napproaches that require a significant amount of training examples. When\ncompared to the RoBERTa baselines, our best approach establishes\nstate-of-the-art performance with a 7% improvement in Exact Match.\n",
        "authors": "Felix Virgo, Fei Cheng, Lis Kanashiro Pereira, Masayuki Asahara,\n  Ichiro Kobayashi and Sadao Kurohashi",
        "authors_parsed": [
            [
                "Virgo",
                "Felix",
                ""
            ],
            [
                "Cheng",
                "Fei",
                ""
            ],
            [
                "Pereira",
                "Lis Kanashiro",
                ""
            ],
            [
                "Asahara",
                "Masayuki",
                ""
            ],
            [
                "Kobayashi",
                "Ichiro",
                ""
            ],
            [
                "Kurohashi",
                "Sadao",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": null,
        "doi": null,
        "id": "2403.18504",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Fei Cheng",
        "title": "AcTED: Automatic Acquisition of Typical Event Duration for\n  Semi-supervised Temporal Commonsense QA",
        "update_date": "2024-03-28",
        "versions": [
            {
                "created": "Wed, 27 Mar 2024 12:33:42 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Research suggests that providing specific and timely feedback to human tutors\nenhances their performance. However, it presents challenges due to the\ntime-consuming nature of assessing tutor performance by human evaluators. Large\nlanguage models, such as the AI-chatbot ChatGPT, hold potential for offering\nconstructive feedback to tutors in practical settings. Nevertheless, the\naccuracy of AI-generated feedback remains uncertain, with scant research\ninvestigating the ability of models like ChatGPT to deliver effective feedback.\nIn this work-in-progress, we evaluate 30 dialogues generated by GPT-4 in a\ntutor-student setting. We use two different prompting approaches, the zero-shot\nchain of thought and the few-shot chain of thought, to identify specific\ncomponents of effective praise based on five criteria. These approaches are\nthen compared to the results of human graders for accuracy. Our goal is to\nassess the extent to which GPT-4 can accurately identify each praise criterion.\nWe found that both zero-shot and few-shot chain of thought approaches yield\ncomparable results. GPT-4 performs moderately well in identifying instances\nwhen the tutor offers specific and immediate praise. However, GPT-4\nunderperforms in identifying the tutor's ability to deliver sincere praise,\nparticularly in the zero-shot prompting scenario where examples of sincere\ntutor praise statements were not provided. Future work will focus on enhancing\nprompt engineering, developing a more general tutoring rubric, and evaluating\nour method using real-life tutoring dialogues.\n",
        "authors": "Dollaya Hirunyasiri, Danielle R. Thomas, Jionghao Lin, Kenneth R.\n  Koedinger, Vincent Aleven",
        "authors_parsed": [
            [
                "Hirunyasiri",
                "Dollaya",
                ""
            ],
            [
                "Thomas",
                "Danielle R.",
                ""
            ],
            [
                "Lin",
                "Jionghao",
                ""
            ],
            [
                "Koedinger",
                "Kenneth R.",
                ""
            ],
            [
                "Aleven",
                "Vincent",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI cs.HC",
        "comments": "12 pages Workshop paper, The 24th International Conference on\n  Artificial Intelligence in Education, AIED 2023 Educational Dialogue Act\n  Classification, Large Language Models, Tutor Training",
        "doi": null,
        "id": "2307.02018",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Jionghao Lin",
        "title": "Comparative Analysis of GPT-4 and Human Graders in Evaluating Praise\n  Given to Students in Synthetic Dialogues",
        "update_date": "2023-07-06",
        "versions": [
            {
                "created": "Wed, 5 Jul 2023 04:14:01 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Transformers achieve state-of-the-art accuracy and robustness across many\ntasks, but an understanding of the inductive biases that they have and how\nthose biases are different from other neural network architectures remains\nelusive. Various neural network architectures such as fully connected networks\nhave been found to have a simplicity bias towards simple functions of the data;\none version of this simplicity bias is a spectral bias to learn simple\nfunctions in the Fourier space. In this work, we identify the notion of\nsensitivity of the model to random changes in the input as a notion of\nsimplicity bias which provides a unified metric to explain the simplicity and\nspectral bias of transformers across different data modalities. We show that\ntransformers have lower sensitivity than alternative architectures, such as\nLSTMs, MLPs and CNNs, across both vision and language tasks. We also show that\nlow-sensitivity bias correlates with improved robustness; furthermore, it can\nalso be used as an efficient intervention to further improve the robustness of\ntransformers.\n",
        "authors": "Bhavya Vasudeva, Deqing Fu, Tianyi Zhou, Elliott Kau, Youqi Huang,\n  Vatsal Sharan",
        "authors_parsed": [
            [
                "Vasudeva",
                "Bhavya",
                ""
            ],
            [
                "Fu",
                "Deqing",
                ""
            ],
            [
                "Zhou",
                "Tianyi",
                ""
            ],
            [
                "Kau",
                "Elliott",
                ""
            ],
            [
                "Huang",
                "Youqi",
                ""
            ],
            [
                "Sharan",
                "Vatsal",
                ""
            ]
        ],
        "categories": "cs.LG cs.AI cs.CL stat.ML",
        "comments": "24 pages, 19 figures, 3 tables",
        "doi": null,
        "id": "2403.06925",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "report-no": null,
        "submitter": "Bhavya Vasudeva",
        "title": "Simplicity Bias of Transformers to Learn Low Sensitivity Functions",
        "update_date": "2024-03-12",
        "versions": [
            {
                "created": "Mon, 11 Mar 2024 17:12:09 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  The springing up of Large Language Models (LLMs) has shifted the community\nfrom single-task-orientated natural language processing (NLP) research to a\nholistic end-to-end multi-task learning paradigm. Along this line of research\nendeavors in the area, LLM-based prompting methods have attracted much\nattention, partially due to the technological advantages brought by prompt\nengineering (PE) as well as the underlying NLP principles disclosed by various\nprompting methods. Traditional supervised learning usually requires training a\nmodel based on labeled data and then making predictions. In contrast, PE\nmethods directly use the powerful capabilities of existing LLMs (i.e., GPT-3\nand GPT-4) via composing appropriate prompts, especially under few-shot or\nzero-shot scenarios. Facing the abundance of studies related to the prompting\nand the ever-evolving nature of this field, this article aims to (i) illustrate\na novel perspective to review existing PE methods, within the well-established\ncommunication theory framework; (ii) facilitate a better/deeper understanding\nof developing trends of existing PE methods used in four typical tasks; (iii)\nshed light on promising research directions for future PE methods.\n",
        "authors": "Yuanfeng Song, Yuanqin He, Xuefang Zhao, Hanlin Gu, Di Jiang, Haijun\n  Yang, Lixin Fan, Qiang Yang",
        "authors_parsed": [
            [
                "Song",
                "Yuanfeng",
                ""
            ],
            [
                "He",
                "Yuanqin",
                ""
            ],
            [
                "Zhao",
                "Xuefang",
                ""
            ],
            [
                "Gu",
                "Hanlin",
                ""
            ],
            [
                "Jiang",
                "Di",
                ""
            ],
            [
                "Yang",
                "Haijun",
                ""
            ],
            [
                "Fan",
                "Lixin",
                ""
            ],
            [
                "Yang",
                "Qiang",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI",
        "comments": null,
        "doi": null,
        "id": "2310.18358",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Yuanfeng Song",
        "title": "A Communication Theory Perspective on Prompting Engineering Methods for\n  Large Language Models",
        "update_date": "2023-10-31",
        "versions": [
            {
                "created": "Tue, 24 Oct 2023 03:05:21 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Recurrent Neural Networks (RNNs) have become the standard modeling technique\nfor sequence data, and are used in a number of novel text-to-speech models.\nHowever, training a TTS model including RNN components has certain requirements\nfor GPU performance and takes a long time. In contrast, studies have shown that\nCNN-based sequence synthesis technology can greatly reduce training time in\ntext-to-speech models while ensuring a certain performance due to its high\nparallelism. We propose a new text-to-speech system based on deep convolutional\nneural networks that does not employ any RNN components (recurrent units). At\nthe same time, we improve the generality and robustness of our model through a\nseries of data augmentation methods such as Time Warping, Frequency Mask, and\nTime Mask. The final experimental results show that the TTS model using only\nthe CNN component can reduce the training time compared to the classic TTS\nmodels such as Tacotron while ensuring the quality of the synthesized speech.\n",
        "authors": "Ziqi Liang",
        "authors_parsed": [
            [
                "Liang",
                "Ziqi",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI",
        "comments": null,
        "doi": null,
        "id": "2211.01948",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Ziqi Liang",
        "title": "Efficiently Trained Low-Resource Mongolian Text-to-Speech System Based\n  On FullConv-TTS",
        "update_date": "2023-04-18",
        "versions": [
            {
                "created": "Mon, 24 Oct 2022 14:18:43 GMT",
                "version": "v1"
            },
            {
                "created": "Wed, 30 Nov 2022 10:01:43 GMT",
                "version": "v2"
            },
            {
                "created": "Sun, 16 Apr 2023 05:01:29 GMT",
                "version": "v3"
            }
        ]
    },
    {
        "abstract": "  Exploring the capabilities of Large Language Models (LLMs) in puzzle solving\nunveils critical insights into their potential and challenges in artificial\nintelligence, marking a significant step towards understanding their\napplicability in complex reasoning tasks. This survey leverages a unique\ntaxonomy -- dividing puzzles into rule-based and rule-less categories -- to\ncritically assess LLMs through various methodologies, including prompting\ntechniques, neuro-symbolic approaches, and fine-tuning. Through a critical\nreview of relevant datasets and benchmarks, we assess LLMs' performance,\nidentifying significant challenges in complex puzzle scenarios. Our findings\nhighlight the disparity between LLM capabilities and human-like reasoning,\nparticularly in those requiring advanced logical inference. The survey\nunderscores the necessity for novel strategies and richer datasets to advance\nLLMs' puzzle-solving proficiency and contribute to AI's logical reasoning and\ncreative problem-solving advancements.\n",
        "authors": "Panagiotis Giadikiaroglou, Maria Lymperaiou, Giorgos Filandrianos,\n  Giorgos Stamou",
        "authors_parsed": [
            [
                "Giadikiaroglou",
                "Panagiotis",
                ""
            ],
            [
                "Lymperaiou",
                "Maria",
                ""
            ],
            [
                "Filandrianos",
                "Giorgos",
                ""
            ],
            [
                "Stamou",
                "Giorgos",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI",
        "comments": null,
        "doi": null,
        "id": "2402.11291",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
        "report-no": null,
        "submitter": "Maria Lymperaiou",
        "title": "Puzzle Solving using Reasoning of Large Language Models: A Survey",
        "update_date": "2024-02-20",
        "versions": [
            {
                "created": "Sat, 17 Feb 2024 14:19:38 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  During the diagnostic process, clinicians leverage multimodal information,\nsuch as chief complaints, medical images, and laboratory-test results.\nDeep-learning models for aiding diagnosis have yet to meet this requirement.\nHere we report a Transformer-based representation-learning model as a clinical\ndiagnostic aid that processes multimodal input in a unified manner. Rather than\nlearning modality-specific features, the model uses embedding layers to convert\nimages and unstructured and structured text into visual tokens and text tokens,\nand bidirectional blocks with intramodal and intermodal attention to learn a\nholistic representation of radiographs, the unstructured chief complaint and\nclinical history, structured clinical information such as laboratory-test\nresults and patient demographic information. The unified model outperformed an\nimage-only model and non-unified multimodal diagnosis models in the\nidentification of pulmonary diseases (by 12% and 9%, respectively) and in the\nprediction of adverse clinical outcomes in patients with COVID-19 (by 29% and\n7%, respectively). Leveraging unified multimodal Transformer-based models may\nhelp streamline triage of patients and facilitate the clinical decision\nprocess.\n",
        "authors": "Hong-Yu Zhou, Yizhou Yu, Chengdi Wang, Shu Zhang, Yuanxu Gao, Jia Pan,\n  Jun Shao, Guangming Lu, Kang Zhang, Weimin Li",
        "authors_parsed": [
            [
                "Zhou",
                "Hong-Yu",
                ""
            ],
            [
                "Yu",
                "Yizhou",
                ""
            ],
            [
                "Wang",
                "Chengdi",
                ""
            ],
            [
                "Zhang",
                "Shu",
                ""
            ],
            [
                "Gao",
                "Yuanxu",
                ""
            ],
            [
                "Pan",
                "Jia",
                ""
            ],
            [
                "Shao",
                "Jun",
                ""
            ],
            [
                "Lu",
                "Guangming",
                ""
            ],
            [
                "Zhang",
                "Kang",
                ""
            ],
            [
                "Li",
                "Weimin",
                ""
            ]
        ],
        "categories": "cs.CV cs.CL cs.LG",
        "comments": "Accepted by Nature Biomedical Engineering",
        "doi": null,
        "id": "2306.00864",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Hong-Yu Zhou",
        "title": "A Transformer-based representation-learning model with unified\n  processing of multimodal input for clinical diagnostics",
        "update_date": "2023-06-02",
        "versions": [
            {
                "created": "Thu, 1 Jun 2023 16:23:47 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Human languages vary widely in how they encode information within\ncircumscribed semantic domains (e.g., time, space, color, human body parts and\nactivities), but little is known about the global structure of semantic\ninformation and nothing about its relation to human communication. We first\nshow that across a sample of ~1,000 languages, there is broad variation in how\ndensely languages encode information into their words. Second, we show that\nthis language information density is associated with a denser configuration of\nsemantic information. Finally, we trace the relationship between language\ninformation density and patterns of communication, showing that informationally\ndenser languages tend toward (1) faster communication, but (2) conceptually\nnarrower conversations within which topics of conversation are discussed at\ngreater depth. These results highlight an important source of variation across\nthe human communicative channel, revealing that the structure of language\nshapes the nature and texture of human engagement, with consequences for human\nbehavior across levels of society.\n",
        "authors": "Pedro Aceves and James A. Evans",
        "authors_parsed": [
            [
                "Aceves",
                "Pedro",
                ""
            ],
            [
                "Evans",
                "James A.",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "Nat Hum Behav (2024)",
        "doi": "10.1038/s41562-024-01815-w",
        "id": "2112.08491",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Pedro Aceves",
        "title": "Human Languages with Greater Information Density Increase Communication\n  Speed, but Decrease Conversation Breadth",
        "update_date": "2024-02-19",
        "versions": [
            {
                "created": "Wed, 15 Dec 2021 21:35:56 GMT",
                "version": "v1"
            },
            {
                "created": "Fri, 29 Sep 2023 14:28:17 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Having an intelligent dialogue agent that can engage in conversational\nquestion answering (ConvQA) is now no longer limited to Sci-Fi movies only and\nhas, in fact, turned into a reality. These intelligent agents are required to\nunderstand and correctly interpret the sequential turns provided as the context\nof the given question. However, these sequential questions are sometimes left\nimplicit and thus require the resolution of some natural language phenomena\nsuch as anaphora and ellipsis. The task of question rewriting has the potential\nto address the challenges of resolving dependencies amongst the contextual\nturns by transforming them into intent-explicit questions. Nonetheless, the\nsolution of rewriting the implicit questions comes with some potential\nchallenges such as resulting in verbose questions and taking conversational\naspect out of the scenario by generating self-contained questions. In this\npaper, we propose a novel framework, CONVSR (CONVQA using Structured\nRepresentations) for capturing and generating intermediate representations as\nconversational cues to enhance the capability of the QA model to better\ninterpret the incomplete questions. We also deliberate how the strengths of\nthis task could be leveraged in a bid to design more engaging and eloquent\nconversational agents. We test our model on the QuAC and CANARD datasets and\nillustrate by experimental results that our proposed framework achieves a\nbetter F1 score than the standard question rewriting model.\n",
        "authors": "Munazza Zaib and Quan Z. Sheng and Wei Emma Zhang and Adnan Mahmood",
        "authors_parsed": [
            [
                "Zaib",
                "Munazza",
                ""
            ],
            [
                "Sheng",
                "Quan Z.",
                ""
            ],
            [
                "Zhang",
                "Wei Emma",
                ""
            ],
            [
                "Mahmood",
                "Adnan",
                ""
            ]
        ],
        "categories": "cs.CL cs.IR",
        "comments": null,
        "doi": null,
        "id": "2304.07125",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Munazza Zaib Ms.",
        "title": "Keeping the Questions Conversational: Using Structured Representations\n  to Resolve Dependency in Conversational Question Answering",
        "update_date": "2023-04-17",
        "versions": [
            {
                "created": "Fri, 14 Apr 2023 13:42:32 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  As large language models are integrated into society, robustness toward a\nsuite of prompts is increasingly important to maintain reliability in a\nhigh-variance environment.Robustness evaluations must comprehensively\nencapsulate the various settings in which a user may invoke an intelligent\nsystem. This paper proposes ASSERT, Automated Safety Scenario Red Teaming,\nconsisting of three methods -- semantically aligned augmentation, target\nbootstrapping, and adversarial knowledge injection. For robust safety\nevaluation, we apply these methods in the critical domain of AI safety to\nalgorithmically generate a test suite of prompts covering diverse robustness\nsettings -- semantic equivalence, related scenarios, and adversarial. We\npartition our prompts into four safety domains for a fine-grained analysis of\nhow the domain affects model performance. Despite dedicated safeguards in\nexisting state-of-the-art models, we find statistically significant performance\ndifferences of up to 11% in absolute classification accuracy among semantically\nrelated scenarios and error rates of up to 19% absolute error in zero-shot\nadversarial settings, raising concerns for users' physical safety.\n",
        "authors": "Alex Mei, Sharon Levy, William Yang Wang",
        "authors_parsed": [
            [
                "Mei",
                "Alex",
                ""
            ],
            [
                "Levy",
                "Sharon",
                ""
            ],
            [
                "Wang",
                "William Yang",
                ""
            ]
        ],
        "categories": "cs.CL cs.AI cs.LG",
        "comments": "In Findings of the 2023 Conference on Empirical Methods in Natural\n  Language Processing",
        "doi": null,
        "id": "2310.09624",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Alex Mei",
        "title": "ASSERT: Automated Safety Scenario Red Teaming for Evaluating the\n  Robustness of Large Language Models",
        "update_date": "2023-11-14",
        "versions": [
            {
                "created": "Sat, 14 Oct 2023 17:10:28 GMT",
                "version": "v1"
            },
            {
                "created": "Sat, 11 Nov 2023 05:30:34 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  LLMs have marked a revolutonary shift, yet they falter when faced with\ncompositional reasoning tasks. Our research embarks on a quest to uncover the\nroot causes of compositional reasoning failures of LLMs, uncovering that most\nof them stem from the improperly generated or leveraged implicit reasoning\nresults. Inspired by our empirical findings, we resort to Logit Lens and an\nintervention experiment to dissect the inner hidden states of LLMs. This deep\ndive reveals that implicit reasoning results indeed surface within middle\nlayers and play a causative role in shaping the final explicit reasoning\nresults. Our exploration further locates multi-head self-attention (MHSA)\nmodules within these layers, which emerge as the linchpins in accurate\ngeneration and leveraing of implicit reasoning results. Grounded on the above\nfindings, we develop CREME, a lightweight method to patch errors in\ncompositional reasoning via editing the located MHSA modules. Our empirical\nevidence stands testament to CREME's effectiveness, paving the way for\nautonomously and continuously enhancing compositional reasoning capabilities in\nlanguage models.\n",
        "authors": "Zhaoyi Li, Gangwei Jiang, Hong Xie, Linqi Song, Defu Lian, Ying Wei",
        "authors_parsed": [
            [
                "Li",
                "Zhaoyi",
                ""
            ],
            [
                "Jiang",
                "Gangwei",
                ""
            ],
            [
                "Xie",
                "Hong",
                ""
            ],
            [
                "Song",
                "Linqi",
                ""
            ],
            [
                "Lian",
                "Defu",
                ""
            ],
            [
                "Wei",
                "Ying",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "Work In Progress",
        "doi": null,
        "id": "2402.14328",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Zhaoyi Li",
        "title": "Understanding and Patching Compositional Reasoning in LLMs",
        "update_date": "2024-02-23",
        "versions": [
            {
                "created": "Thu, 22 Feb 2024 06:47:56 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  The breakthrough performance of large language models (LLMs) comes with major\ncomputational footprints and high deployment costs. In this paper, we progress\ntowards resolving this problem by proposing a novel structured compression\napproach for LLMs, called ZipLM. ZipLM achieves state-of-the-art\naccuracy-vs-speedup, while matching a set of desired target runtime speedups in\nany given inference environment. Specifically, given a model, a dataset, an\ninference environment, as well as a set of speedup targets, ZipLM iteratively\nidentifies and removes components with the worst loss-runtime trade-off. Unlike\nprior methods that specialize in either the post-training/one-shot or the\ngradual compression setting, and only for specific families of models such as\nBERT (encoder) or GPT (decoder), ZipLM produces state-of-the-art compressed\nmodels across all these settings. Furthermore, ZipLM achieves superior results\nfor a fraction of the computational cost relative to prior distillation and\npruning techniques, making it a cost-effective approach for generating an\nentire family of smaller, faster, and highly accurate models, guaranteed to\nmeet the desired inference specifications. In particular, ZipLM outperforms all\nprior BERT-base distillation and pruning techniques, such as CoFi, MiniLM, and\nTinyBERT. Moreover, it matches the performance of the heavily optimized\nMobileBERT model, obtained via extensive architecture search, by simply pruning\nthe baseline BERT-large model. When compressing GPT2, ZipLM outperforms\nDistilGPT2 while being 60% smaller and 30% faster. Our code is available at:\nhttps://github.com/IST-DASLab/ZipLM.\n",
        "authors": "Eldar Kurtic, Elias Frantar, Dan Alistarh",
        "authors_parsed": [
            [
                "Kurtic",
                "Eldar",
                ""
            ],
            [
                "Frantar",
                "Elias",
                ""
            ],
            [
                "Alistarh",
                "Dan",
                ""
            ]
        ],
        "categories": "cs.LG cs.CL",
        "comments": "Accepted to NeurIPS 2023",
        "doi": null,
        "id": "2302.04089",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Eldar Kurtic",
        "title": "ZipLM: Inference-Aware Structured Pruning of Language Models",
        "update_date": "2023-10-27",
        "versions": [
            {
                "created": "Tue, 7 Feb 2023 18:55:28 GMT",
                "version": "v1"
            },
            {
                "created": "Thu, 26 Oct 2023 06:42:40 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  The rapid spread of fake news is a serious problem calling for AI solutions.\nWe employ a deep learning based automated detector through a three level\nhierarchical attention network (3HAN) for fast, accurate detection of fake\nnews. 3HAN has three levels, one each for words, sentences, and the headline,\nand constructs a news vector: an effective representation of an input news\narticle, by processing an article in an hierarchical bottom-up manner. The\nheadline is known to be a distinguishing feature of fake news, and furthermore,\nrelatively few words and sentences in an article are more important than the\nrest. 3HAN gives a differential importance to parts of an article, on account\nof its three layers of attention. By experiments on a large real-world data\nset, we observe the effectiveness of 3HAN with an accuracy of 96.77%. Unlike\nsome other deep learning models, 3HAN provides an understandable output through\nthe attention weights given to different parts of an article, which can be\nvisualized through a heatmap to enable further manual fact checking.\n",
        "authors": "Sneha Singhania, Nigel Fernandez, Shrisha Rao",
        "authors_parsed": [
            [
                "Singhania",
                "Sneha",
                ""
            ],
            [
                "Fernandez",
                "Nigel",
                ""
            ],
            [
                "Rao",
                "Shrisha",
                ""
            ]
        ],
        "categories": "cs.LG cs.CL cs.SI",
        "comments": "Published as a conference paper at ICONIP 2017",
        "doi": "10.1007/978-3-319-70096-0_59",
        "id": "2306.12014",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Nigel Fernandez",
        "title": "3HAN: A Deep Neural Network for Fake News Detection",
        "update_date": "2023-06-22",
        "versions": [
            {
                "created": "Wed, 21 Jun 2023 04:34:27 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Many text mining models are constructed by fine-tuning a large deep\npre-trained language model (PLM) in downstream tasks. However, a significant\nchallenge nowadays is maintaining performance when we use a lightweight model\nwith limited labelled samples. We present DisCo, a semi-supervised learning\n(SSL) framework for fine-tuning a cohort of small student models generated from\na large PLM using knowledge distillation. Our key insight is to share\ncomplementary knowledge among distilled student cohorts to promote their SSL\neffectiveness. DisCo employs a novel co-training technique to optimize a cohort\nof multiple small student models by promoting knowledge sharing among students\nunder diversified views: model views produced by different distillation\nstrategies and data views produced by various input augmentations. We evaluate\nDisCo on both semi-supervised text classification and extractive summarization\ntasks. Experimental results show that DisCo can produce student models that are\n7.6 times smaller and 4.8 times faster in inference than the baseline PLMs\nwhile maintaining comparable performance. We also show that DisCo-generated\nstudent models outperform the similar-sized models elaborately tuned in\ndistinct tasks.\n",
        "authors": "Weifeng Jiang, Qianren Mao, Chenghua Lin, Jianxin Li, Ting Deng, Weiyi\n  Yang and Zheng Wang",
        "authors_parsed": [
            [
                "Jiang",
                "Weifeng",
                ""
            ],
            [
                "Mao",
                "Qianren",
                ""
            ],
            [
                "Lin",
                "Chenghua",
                ""
            ],
            [
                "Li",
                "Jianxin",
                ""
            ],
            [
                "Deng",
                "Ting",
                ""
            ],
            [
                "Yang",
                "Weiyi",
                ""
            ],
            [
                "Wang",
                "Zheng",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": null,
        "doi": null,
        "id": "2305.12074",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Weifeng Jiang",
        "title": "DisCo: Distilled Student Models Co-training for Semi-supervised Text\n  Mining",
        "update_date": "2023-10-23",
        "versions": [
            {
                "created": "Sat, 20 May 2023 03:23:16 GMT",
                "version": "v1"
            },
            {
                "created": "Mon, 16 Oct 2023 11:39:50 GMT",
                "version": "v2"
            },
            {
                "created": "Fri, 20 Oct 2023 02:11:16 GMT",
                "version": "v3"
            }
        ]
    },
    {
        "abstract": "  Graphs data is crucial for many applications, and much of it exists in the\nrelations described in textual format. As a result, being able to accurately\nrecall and encode a graph described in earlier text is a basic yet pivotal\nability that LLMs need to demonstrate if they are to perform reasoning tasks\nthat involve graph-structured information. Human performance at graph recall\nhas been studied by cognitive scientists for decades, and has been found to\noften exhibit certain structural patterns of bias that align with human\nhandling of social relationships. To date, however, we know little about how\nLLMs behave in analogous graph recall tasks: do their recalled graphs also\nexhibit certain biased patterns, and if so, how do they compare with humans and\naffect other graph reasoning tasks? In this work, we perform the first\nsystematical study of graph recall by LLMs, investigating the accuracy and\nbiased microstructures (local structural patterns) in their recall. We find\nthat LLMs not only underperform often in graph recall, but also tend to favor\nmore triangles and alternating 2-paths. Moreover, we find that more advanced\nLLMs have a striking dependence on the domain that a real-world graph comes\nfrom -- by yielding the best recall accuracy when the graph is narrated in a\nlanguage style consistent with its original domain.\n",
        "authors": "Yanbang Wang, Hejie Cui, Jon Kleinberg",
        "authors_parsed": [
            [
                "Wang",
                "Yanbang",
                ""
            ],
            [
                "Cui",
                "Hejie",
                ""
            ],
            [
                "Kleinberg",
                "Jon",
                ""
            ]
        ],
        "categories": "cs.LG cs.CL cs.IR cs.SI",
        "comments": "16 pages, 7 tables, 5 figures",
        "doi": null,
        "id": "2402.11821",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Yanbang Wang",
        "title": "Microstructures and Accuracy of Graph Recall by Large Language Models",
        "update_date": "2024-02-21",
        "versions": [
            {
                "created": "Mon, 19 Feb 2024 04:29:45 GMT",
                "version": "v1"
            },
            {
                "created": "Tue, 20 Feb 2024 03:23:49 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Teaching an agent to perform new tasks using natural language can easily be\nhindered by ambiguities in interpretation. When a teacher provides an\ninstruction to a learner about an object by referring to its features, the\nlearner can misunderstand the teacher's intentions, for instance if the\ninstruction ambiguously refer to features of the object, a phenomenon called\nreferential ambiguity. We study how two concepts derived from cognitive\nsciences can help resolve those referential ambiguities: pedagogy (selecting\nthe right instructions) and pragmatism (learning the preferences of the other\nagents using inductive reasoning). We apply those ideas to a teacher/learner\nsetup with two artificial agents on a simulated robotic task (block-stacking).\nWe show that these concepts improve sample efficiency for training the learner.\n",
        "authors": "Hugo Caselles-Dupr\\'e, Olivier Sigaud, Mohamed Chetouani",
        "authors_parsed": [
            [
                "Caselles-Dupr\u00e9",
                "Hugo",
                ""
            ],
            [
                "Sigaud",
                "Olivier",
                ""
            ],
            [
                "Chetouani",
                "Mohamed",
                ""
            ]
        ],
        "categories": "cs.LG cs.CL",
        "comments": "NeurIPS 2022 Workshop",
        "doi": null,
        "id": "2209.12758",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Hugo Caselles-Dupr\\'e",
        "title": "Overcoming Referential Ambiguity in Language-Guided Goal-Conditioned\n  Reinforcement Learning",
        "update_date": "2023-09-28",
        "versions": [
            {
                "created": "Mon, 26 Sep 2022 15:07:59 GMT",
                "version": "v1"
            },
            {
                "created": "Wed, 27 Sep 2023 07:52:54 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  Safety of Large Language Models (LLMs) has become a central issue given their\nrapid progress and wide applications. Greedy Coordinate Gradient (GCG) is shown\nto be effective in constructing prompts containing adversarial suffixes to\nbreak the presumingly safe LLMs, but the optimization of GCG is time-consuming\nand limits its practicality. To reduce the time cost of GCG and enable more\ncomprehensive studies of LLM safety, in this work, we study a new algorithm\ncalled $\\texttt{Probe sampling}$ to accelerate the GCG algorithm. At the core\nof the algorithm is a mechanism that dynamically determines how similar a\nsmaller draft model's predictions are to the target model's predictions for\nprompt candidates. When the target model is similar to the draft model, we rely\nheavily on the draft model to filter out a large number of potential prompt\ncandidates to reduce the computation time. Probe sampling achieves up to $5.6$\ntimes speedup using Llama2-7b and leads to equal or improved attack success\nrate (ASR) on the AdvBench.\n",
        "authors": "Yiran Zhao, Wenyue Zheng, Tianle Cai, Xuan Long Do, Kenji Kawaguchi,\n  Anirudh Goyal, Michael Shieh",
        "authors_parsed": [
            [
                "Zhao",
                "Yiran",
                ""
            ],
            [
                "Zheng",
                "Wenyue",
                ""
            ],
            [
                "Cai",
                "Tianle",
                ""
            ],
            [
                "Do",
                "Xuan Long",
                ""
            ],
            [
                "Kawaguchi",
                "Kenji",
                ""
            ],
            [
                "Goyal",
                "Anirudh",
                ""
            ],
            [
                "Shieh",
                "Michael",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": null,
        "doi": null,
        "id": "2403.01251",
        "journal-ref": null,
        "license": "http://creativecommons.org/licenses/by-sa/4.0/",
        "report-no": null,
        "submitter": "Yiran Zhao",
        "title": "Accelerating Greedy Coordinate Gradient via Probe Sampling",
        "update_date": "2024-03-05",
        "versions": [
            {
                "created": "Sat, 2 Mar 2024 16:23:44 GMT",
                "version": "v1"
            }
        ]
    },
    {
        "abstract": "  Automatic detection and severity level classification of dysarthria directly\nfrom acoustic speech signals can be used as a tool in medical diagnosis. In\nthis work, the pre-trained wav2vec 2.0 model is studied as a feature extractor\nto build detection and severity level classification systems for dysarthric\nspeech. The experiments were carried out with the popularly used UA-speech\ndatabase. In the detection experiments, the results revealed that the best\nperformance was obtained using the embeddings from the first layer of the\nwav2vec model that yielded an absolute improvement of 1.23% in accuracy\ncompared to the best performing baseline feature (spectrogram). In the studied\nseverity level classification task, the results revealed that the embeddings\nfrom the final layer gave an absolute improvement of 10.62% in accuracy\ncompared to the best baseline features (mel-frequency cepstral coefficients).\n",
        "authors": "Farhad Javanmardi, Saska Tirronen, Manila Kodali, Sudarsana Reddy\n  Kadiri, Paavo Alku",
        "authors_parsed": [
            [
                "Javanmardi",
                "Farhad",
                ""
            ],
            [
                "Tirronen",
                "Saska",
                ""
            ],
            [
                "Kodali",
                "Manila",
                ""
            ],
            [
                "Kadiri",
                "Sudarsana Reddy",
                ""
            ],
            [
                "Alku",
                "Paavo",
                ""
            ]
        ],
        "categories": "eess.AS cs.CL cs.LG cs.SD eess.SP",
        "comments": "copyright 2023 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
        "doi": "10.1109/ICASSP49357.2023.10094857",
        "id": "2309.14107",
        "journal-ref": "in Proc. ICASSP, Rhodes Island, Greece, June 4-10, 2023",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "report-no": null,
        "submitter": "Sudarsana Kadiri",
        "title": "Wav2vec-based Detection and Severity Level Classification of Dysarthria\n  from Speech",
        "update_date": "2023-10-18",
        "versions": [
            {
                "created": "Mon, 25 Sep 2023 13:00:33 GMT",
                "version": "v1"
            },
            {
                "created": "Tue, 17 Oct 2023 13:38:27 GMT",
                "version": "v2"
            }
        ]
    },
    {
        "abstract": "  We present the Granular AMR Parsing Evaluation Suite (GrAPES), a challenge\nset for Abstract Meaning Representation (AMR) parsing with accompanying\nevaluation metrics. AMR parsers now obtain high scores on the standard AMR\nevaluation metric Smatch, close to or even above reported inter-annotator\nagreement. But that does not mean that AMR parsing is solved; in fact, human\nevaluation in previous work indicates that current parsers still quite\nfrequently make errors on node labels or graph structure that substantially\ndistort sentence meaning. Here, we provide an evaluation suite that tests AMR\nparsers on a range of phenomena of practical, technical, and linguistic\ninterest. Our 36 categories range from seen and unseen labels, to structural\ngeneralization, to coreference. GrAPES reveals in depth the abilities and\nshortcomings of current AMR parsers.\n",
        "authors": "Jonas Groschwitz, Shay B. Cohen, Lucia Donatelli, Meaghan Fowlie",
        "authors_parsed": [
            [
                "Groschwitz",
                "Jonas",
                ""
            ],
            [
                "Cohen",
                "Shay B.",
                ""
            ],
            [
                "Donatelli",
                "Lucia",
                ""
            ],
            [
                "Fowlie",
                "Meaghan",
                ""
            ]
        ],
        "categories": "cs.CL",
        "comments": "Accepted at EMNLP 2023. For the associated GitHub repository, see\n  https://github.com/jgroschwitz/GrAPES",
        "doi": null,
        "id": "2312.03480",
        "journal-ref": null,
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "report-no": null,
        "submitter": "Jonas Groschwitz",
        "title": "AMR Parsing is Far from Solved: GrAPES, the Granular AMR Parsing\n  Evaluation Suite",
        "update_date": "2023-12-07",
        "versions": [
            {
                "created": "Wed, 6 Dec 2023 13:19:56 GMT",
                "version": "v1"
            }
        ]
    }
]